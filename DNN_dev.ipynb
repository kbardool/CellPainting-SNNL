{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1543eaa-61c1-4b00-9745-9db4239b5b15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T10:29:16.111588Z",
     "start_time": "2023-04-12T10:29:15.764305Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:22.772705Z",
     "iopub.status.busy": "2024-04-05T22:41:22.772225Z",
     "iopub.status.idle": "2024-04-05T22:41:22.800740Z",
     "shell.execute_reply": "2024-04-05T22:41:22.799943Z",
     "shell.execute_reply.started": "2024-04-05T22:41:22.772661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "%load_ext autoreload  \n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ccba311-11f9-491c-86d8-1bd8b3c1803e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T10:29:16.111588Z",
     "start_time": "2023-04-12T10:29:15.764305Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:22.802368Z",
     "iopub.status.busy": "2024-04-05T22:41:22.802144Z",
     "iopub.status.idle": "2024-04-05T22:41:22.823466Z",
     "shell.execute_reply": "2024-04-05T22:41:22.822888Z",
     "shell.execute_reply.started": "2024-04-05T22:41:22.802346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./src', '/home/kevin/miniforge3/envs/ptsnnl/lib/python311.zip', '/home/kevin/miniforge3/envs/ptsnnl/lib/python3.11', '/home/kevin/miniforge3/envs/ptsnnl/lib/python3.11/lib-dynload', '', '/home/kevin/miniforge3/envs/ptsnnl/lib/python3.11/site-packages']\n",
      "2.2.1+cu118\n",
      "7- '%Y_%m_%d_%H:%M:%S'   :  20240406_004122\n",
      " Timestamp:  ./tmp/sql_cohort_2024_04_06_00:41:22.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import types, copy, pprint\n",
    "import logging \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "if './src' not in sys.path:\n",
    "    print(f\"insert ./src\")\n",
    "    sys.path.insert(0, './src')\n",
    "print(sys.path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "from scipy.sparse import csr_matrix\n",
    "import shutil\n",
    "import getpass\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "print(torch.__version__)\n",
    "\n",
    "pd.options.display.width = 132\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=150, profile=None, sci_mode=None)\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Adashare_Train.ipynb\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# print(\"1- '%x_%X'               : \",time.strftime('%x_%X'))\n",
    "# print(\"2- '%X.%f'               : \",datetime.now().strftime('%X.%f'))\n",
    "# print(\"3- '%X %x %Z'            : \",time.strftime('%X %x %Z'))\n",
    "# print(\"4- '%D-%X.%f'            : \",datetime.now().strftime('%D-%X.%f'))\n",
    "# print(\"5- '%Y-%m-%d %H:%M:%S.%f': \",datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "# print(\"6- '%Y%m%d_%H%M%S'       : \",datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "# print(\"7- '%Y_%m_%d_%H:%M:%S'   : \",datetime.now().strftime('%Y_%m_%d_%H:%M:%S'))\n",
    "print(\"7- '%Y_%m_%d_%H:%M:%S'   : \",datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y_%m_%d_%H:%M:%S')\n",
    "print(' Timestamp: ','./tmp/sql_cohort'+'_'+timestamp+'.txt')\n",
    "\n",
    "logLevel = os.environ.get('LOG_LEVEL', 'INFO').upper()\n",
    "FORMAT = '%(asctime)s - %(levelname)s: - %(message)s'\n",
    "logging.basicConfig(level=\"INFO\", format= FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8059a7a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:22.824516Z",
     "iopub.status.busy": "2024-04-05T22:41:22.824295Z",
     "iopub.status.idle": "2024-04-05T22:41:22.841067Z",
     "shell.execute_reply": "2024-04-05T22:41:22.840319Z",
     "shell.execute_reply.started": "2024-04-05T22:41:22.824494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda:0 Quadro GV100 0\n",
      "Current device is : cuda:0\n"
     ]
    }
   ],
   "source": [
    "def _get_device():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = f\"{device}:{torch.cuda.current_device()}\"\n",
    "    print(\"Running on:\", device, torch.cuda.get_device_name(), torch.cuda.current_device())\n",
    "    return device\n",
    "\n",
    "current_device = _get_device()\n",
    "print(f\"Current device is : {current_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a51595-5eb1-4169-83e1-ab476383de22",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd74b69c-7c4c-42cf-ae66-1dd402d5e341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:22.843135Z",
     "iopub.status.busy": "2024-04-05T22:41:22.842861Z",
     "iopub.status.idle": "2024-04-05T22:41:22.862093Z",
     "shell.execute_reply": "2024-04-05T22:41:22.861368Z",
     "shell.execute_reply.started": "2024-04-05T22:41:22.843113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample module for using DNN classifier with SNNL'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Soft Nearest Neighbor Loss\n",
    "# Copyright (C) 2020  Abien Fred Agarap\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero General Public License as published\n",
    "# by the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\"\"\"Sample module for using DNN classifier with SNNL\"\"\"\n",
    "# import argparse\n",
    "# import torch\n",
    "# from pt_datasets import create_dataloader, load_dataset\n",
    "# from pt_datasets import display_cellpainting_batch\n",
    "# from snnl.models import DNN\n",
    "from snnl.utils import export_results, get_hyperparameters, set_global_seed\n",
    "from snnl.utils.metrics import accuracy\n",
    "from snnl.utils import load_model, save_model, import_results, export_results, _save_checkpoint, _load_previous_state\n",
    "\n",
    "__author__ = \"Abien Fred Agarap\"\n",
    "__version__ = \"1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c53e68ae-5323-4691-aacc-b79d5f8ede2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:22.863154Z",
     "iopub.status.busy": "2024-04-05T22:41:22.862969Z",
     "iopub.status.idle": "2024-04-05T22:41:25.235412Z",
     "shell.execute_reply": "2024-04-05T22:41:25.234650Z",
     "shell.execute_reply.started": "2024-04-05T22:41:22.863135Z"
    }
   },
   "outputs": [],
   "source": [
    "from dev_code import Model, DNN, Autoencoder, SNNLoss\n",
    "from dev_code import CellpaintingDataset, InfiniteDataLoader, custom_collate_fn, parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39bf86db-590d-49b6-9ddb-b66eab08e698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:25.237253Z",
     "iopub.status.busy": "2024-04-05T22:41:25.236794Z",
     "iopub.status.idle": "2024-04-05T22:41:25.272142Z",
     "shell.execute_reply": "2024-04-05T22:41:25.271524Z",
     "shell.execute_reply.started": "2024-04-05T22:41:25.237217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(seed=1234, model='snnl', configuration='examples/hyperparameters/dnn_cellpainting_unsupervised.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "input_args = f\" --seed              1234 \" \\\n",
    "             f\" --model             snnl\" \\\n",
    "             f\" --configuration     examples/hyperparameters/dnn_cellpainting_unsupervised.json\"\n",
    "             # f\" --configuration     examples/hyperparameters/dnn_cellpainting_classifier.json\"\n",
    "             # f\" --configuration     examples/hyperparameters/dnn_mnist.json\"\n",
    "             # f\" --model             baseline\" \\\n",
    "args = parse_args(input_args.split())\n",
    "args\n",
    "set_global_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5e789-3446-482f-a3ca-fdd19b2a845e",
   "metadata": {},
   "source": [
    "### main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed4ddef2-30f2-45c3-b5ff-c7def50ddaaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:25.273507Z",
     "iopub.status.busy": "2024-04-05T22:41:25.273228Z",
     "iopub.status.idle": "2024-04-05T22:41:25.305668Z",
     "shell.execute_reply": "2024-04-05T22:41:25.304986Z",
     "shell.execute_reply.started": "2024-04-05T22:41:25.273486Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello common cellpainting hyperparameters\n",
      "loading dnn hyperparameters\n",
      "loading dnn cellpainting hyperparameters\n"
     ]
    }
   ],
   "source": [
    "# DNN\n",
    "(   dataset,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    learning_rate,\n",
    "    units,\n",
    "    snnl_factor,\n",
    "    temperature,\n",
    "    cellpainting_args\n",
    " ) = get_hyperparameters(args.configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb5af3a-5c2c-482e-b133-eff26ad0b812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:25.307396Z",
     "iopub.status.busy": "2024-04-05T22:41:25.307176Z",
     "iopub.status.idle": "2024-04-05T22:41:25.335020Z",
     "shell.execute_reply": "2024-04-05T22:41:25.334263Z",
     "shell.execute_reply.started": "2024-04-05T22:41:25.307375Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset = 'cellpainting'\n",
    "# units = [[1471, 1024], [1024, 1024], [1024, 512], [512,32] ]\n",
    "# epochs = 10\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "compounds_per_batch = 100\n",
    "temperatureLR = 1e-3\n",
    "# temperature = 1/temperature\n",
    "\n",
    "if dataset == \"cellpainting\": \n",
    "    cellpainting_args['batch_size']  = batch_size\n",
    "    cellpainting_args['dataset_path'] = '/home/kevin/WSL-shared/Cellpainting/cj-datasets/output_11102023/3_sample_profiles/3sample_profiles_1471_HashOrder.csv'\n",
    "    # cellpainting_ds = dict()\n",
    "    # cellpainting_args['train_start'] = 0\n",
    "    cellpainting_args['train_end']     = 120_000\n",
    "    cellpainting_args['compounds_per_batch']   = compounds_per_batch\n",
    "    # cellpainting_args['sample_size'] = 3\n",
    "    # cellpainting_args['test_start']  = 300000\n",
    "    # cellpainting_args['test_end']    = cellpainting_ds['test_start'] + 3333  ## 346542\n",
    "    # cellpainting_args['chunksize']   = None\n",
    "    # cellpainting_args['conversions'] = None\n",
    "    # cellpainting_args['iterator'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7124dbc9-5413-42f2-a16b-4566eec4d3d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:25.336235Z",
     "iopub.status.busy": "2024-04-05T22:41:25.336010Z",
     "iopub.status.idle": "2024-04-05T22:41:25.380865Z",
     "shell.execute_reply": "2024-04-05T22:41:25.380356Z",
     "shell.execute_reply.started": "2024-04-05T22:41:25.336215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Each batch contains 100.0 compounds, 3 samples per each compound : total 300 rows\n",
      " - Number of 300 row full size batches per epoch: 400\n",
      " - Rows covered by 400 full size batches (300 rows) per epoch:  120000\n",
      " - Last batch contains : 0 rows\n"
     ]
    }
   ],
   "source": [
    "file_sz = cellpainting_args['train_end'] - cellpainting_args['train_start']\n",
    "smp_sz = cellpainting_args['sample_size']\n",
    "buf_sz = cellpainting_args['compounds_per_batch']\n",
    "batch_size = 1\n",
    "bth_sz = batch_size\n",
    "\n",
    "recs_per_batch = smp_sz * bth_sz * buf_sz\n",
    "bth_per_epoch = file_sz // recs_per_batch\n",
    "print(f\" - Each batch contains {recs_per_batch/smp_sz} compounds, {smp_sz} samples per each compound : total {recs_per_batch} rows\")\n",
    "print(f\" - Number of {recs_per_batch} row full size batches per epoch: {bth_per_epoch}\")\n",
    "print(f\" - Rows covered by {bth_per_epoch} full size batches ({recs_per_batch} rows) per epoch:  {(file_sz // recs_per_batch) * recs_per_batch}\")\n",
    "print(f\" - Last batch contains : {file_sz % recs_per_batch} rows\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e4f9df4-ca74-4916-8d76-be4bed339b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:25.383111Z",
     "iopub.status.busy": "2024-04-05T22:41:25.382889Z",
     "iopub.status.idle": "2024-04-05T22:41:25.439919Z",
     "shell.execute_reply": "2024-04-05T22:41:25.439207Z",
     "shell.execute_reply.started": "2024-04-05T22:41:25.383091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataset           cellpainting\n",
      "   batch_size        1\n",
      "   epochs            10\n",
      "   learning_rate     0.001\n",
      "   units             [[1471, 1024], [1024, 1024], [1024, 512], [512, 128], [128, 128]]\n",
      "   snnl_factor       10.0\n",
      "   temperature       50.0\n",
      "   temperatureLR:    0.001\n",
      "\n",
      "{   'batch_size': 1,\n",
      "    'chunksize': None,\n",
      "    'compounds_per_batch': 100,\n",
      "    'conversions': None,\n",
      "    'dataset_path': '/home/kevin/WSL-shared/Cellpainting/cj-datasets/output_11102023/3_sample_profiles/3sample_profiles_1471_HashOrder.csv',\n",
      "    'iterator': True,\n",
      "    'sample_size': 3,\n",
      "    'test_end': 310000,\n",
      "    'test_start': 300000,\n",
      "    'train_end': 120000,\n",
      "    'train_start': 0}\n"
     ]
    }
   ],
   "source": [
    "# print(f\"   code_dim          {code_dim}\")\n",
    "print(f\"   dataset           {dataset}\")\n",
    "print(f\"   batch_size        {batch_size}\")\n",
    "print(f\"   epochs            {epochs}\")\n",
    "print(f\"   learning_rate     {learning_rate}\")\n",
    "print(f\"   units             {units}\")\n",
    "print(f\"   snnl_factor       {snnl_factor}\")\n",
    "print(f\"   temperature       {temperature}\")\n",
    "print(f\"   temperatureLR:    {temperatureLR}\")\n",
    "print()\n",
    "if dataset == \"cellpainting\": \n",
    "    pp.pprint(cellpainting_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e7f37-316c-4cce-a0b3-06bba0c9bc27",
   "metadata": {},
   "source": [
    "## Define dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "315953a3-d645-4fe9-8c62-4286ef52f6e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:29.306767Z",
     "iopub.status.busy": "2024-04-05T22:41:29.306269Z",
     "iopub.status.idle": "2024-04-05T22:41:29.345089Z",
     "shell.execute_reply": "2024-04-05T22:41:29.344458Z",
     "shell.execute_reply.started": "2024-04-05T22:41:29.306725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " load cellpainting\n"
     ]
    }
   ],
   "source": [
    "#### Load CellPainting Dataset\n",
    "if dataset == 'cellpainting':\n",
    "    print(f\" load {dataset}\")\n",
    "    train_dataset = CellpaintingDataset(train = True,    **cellpainting_args)\n",
    "    train_loader = InfiniteDataLoader(dataset=train_dataset, batch_size=batch_size,shuffle = False, num_workers = 0, collate_fn = custom_collate_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d19079c4-2bbc-4c50-bf73-cdf549988f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:30.201919Z",
     "iopub.status.busy": "2024-04-05T22:41:30.201708Z",
     "iopub.status.idle": "2024-04-05T22:41:30.231182Z",
     "shell.execute_reply": "2024-04-05T22:41:30.230471Z",
     "shell.execute_reply.started": "2024-04-05T22:41:30.201899Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # train_dataset, test_dataset = load_dataset(name='cellpainting', cellpainting_args = cellpainting_args)\n",
    "    # test_loader = create_dataloader(dataset=test_dataset, batch_size=batch_size, shuffle = False, num_workers = 0 , collate_fn = custom_collate_fn)\n",
    "    # train_dataloader = DataLoader(dataset = train_dataset, batch_size=10, num_workers = 0, collate_fn=custom_collate, worker_init_fn = worker_init_fn, shuffle= False)\n",
    "    # test_dataloader  = DataLoader(dataset = test_dataset , batch_size=12, num_workers = 0, collate_fn=custom_collate, worker_init_fn = worker_init_fn, shuffle= False)\n",
    "\n",
    "# for idx, batch in enumerate(train_loader):\n",
    "#     print(f\"{idx:4d}, {batch[0].shape}, {batch[1].shape},{batch[2].shape},{batch[3].shape},{batch[4].shape}, {batch[3][:3]}, {batch[3][-3:]}\")\n",
    "#     train_batch_id +=1\n",
    "    # display_cellpainting_batch(train_batch_id, batch)\n",
    "    # if idx >= 2:\n",
    "        # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c48ed-c31d-4a58-bde2-36df402aa7b8",
   "metadata": {},
   "source": [
    "### define DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8f7c51c-a2be-4ac8-836a-039ca78084d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:32.778221Z",
     "iopub.status.busy": "2024-04-05T22:41:32.777949Z",
     "iopub.status.idle": "2024-04-05T22:41:33.180909Z",
     "shell.execute_reply": "2024-04-05T22:41:33.180337Z",
     "shell.execute_reply.started": "2024-04-05T22:41:32.778199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " snnl model\n",
      " Building Base Model from NOTEBOOK\n",
      "    Model_init()_    -- Crtierion  :       CrossEntropyLoss()\n",
      "    Model_init()_    -- temperature :      50.0\n",
      "    Model_init()_    -- temperature LR:    0.001\n",
      "    Model_init()_    -- mode:              classifier\n",
      "    Model_init()_    -- unsupervised :     True\n",
      "    Model_init()_    -- use_snnl :         True\n",
      " Build SNNLoss dfrom NOTEBOOK\n",
      " Building SNNLoss from NOTEBOOK\n",
      "    SNNLoss _init()_    -- mode: classifier was found in SNNLoss._supported_modes --   is unsupervised: False\n",
      "    SNNLoss _init()_    -- primary_criterion: CrossEntropyLoss()\n",
      "    SNNLoss _init()_    -- unsupervised :     True\n",
      "    SNNLoss _init()_    -- use_annealing :    False\n",
      "    SNNLoss _init()_    -- sample_size :      3\n",
      " Building DNN from NOTEBOOK\n",
      "    DNN _init()_    -- mode:              classifier\n",
      "    DNN _init()_    -- unsupervised :     True\n",
      "    DNN _init()_    -- use_snnl :         True\n",
      "    DNN _init()_    -- temperature :      Parameter containing:\n",
      "tensor([50.], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.model.lower() == \"baseline\":\n",
    "    print(f\" baseline model\")\n",
    "    model = DNN(units=units, \n",
    "                learning_rate=learning_rate)\n",
    "elif args.model.lower() == \"snnl\":\n",
    "    print(f\" snnl model\")\n",
    "    model = DNN(\n",
    "        units=units,\n",
    "        learning_rate=learning_rate,\n",
    "        use_snnl=True,\n",
    "        factor=snnl_factor,\n",
    "        temperature=temperature,\n",
    "        use_annealing= False,\n",
    "        unsupervised = True,\n",
    "        sample_size = cellpainting_args['sample_size']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Choose between [baseline] and [snnl] only.\")\n",
    "\n",
    "model.device\n",
    "model.optimizer\n",
    "# model.optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "# model.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model.optimizer, mode='min', factor=0.5, patience=30, \n",
    "#                                                              threshold=0.00001, threshold_mode='rel', cooldown=5, \n",
    "#                                                              min_lr=0, eps=1e-08)\n",
    "# model.scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b698f-3de7-462c-8e90-ca82e81b18c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Display model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58157a-3ccb-4041-9526-65647cc27837",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [ \"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"mult_adds\", \"trainable\"]\n",
    "             # \"kernel_size\"\n",
    "summary_input_size = (batch_size * cellpainting_args['sample_size'], 1471)\n",
    "summary(model, input_size=summary_input_size, col_names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47668c14-8c43-4d91-97c1-b1ca48ed6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "for index, layer in enumerate(model.layers):\n",
    "    # print(f\" layer {index}    {layer}\")  \n",
    "    if index == 0:\n",
    "        print(f\"a- layer {index} activations[{index}] : Relu(layer[{index}])                   Input: {layer.in_features}   out: {layer.out_features}\")   \n",
    "    elif index == len(model.layers) - 1:\n",
    "        print(f\"c- layer {index} activations[{index}] : Layer[{index}](activations[{index-1}])         Input: {layer.in_features}   out: {layer.out_features}\")   \n",
    "    else:\n",
    "        print(f\"b- layer {index} activations[{index}] : Relu(Layer[{index}](activations[{index-1}]))   Input: {layer.in_features}   out: {layer.out_features}\")   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1814d083-2e99-49fc-a9bb-7d740f4f37cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T05:38:15.396622Z",
     "iopub.status.busy": "2024-03-26T05:38:15.396404Z",
     "iopub.status.idle": "2024-03-26T05:38:15.425453Z",
     "shell.execute_reply": "2024-03-26T05:38:15.424912Z",
     "shell.execute_reply.started": "2024-03-26T05:38:15.396602Z"
    }
   },
   "source": [
    "### `model.fit(data_loader=train_loader, epochs=epochs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a175996e-d67d-4772-a30d-602d0b8e3b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:36.436779Z",
     "iopub.status.busy": "2024-04-05T22:41:36.436304Z",
     "iopub.status.idle": "2024-04-05T22:41:36.474077Z",
     "shell.execute_reply": "2024-04-05T22:41:36.473515Z",
     "shell.execute_reply.started": "2024-04-05T22:41:36.436736Z"
    }
   },
   "outputs": [],
   "source": [
    "if model.use_snnl:\n",
    "    model.train_snn_loss = []\n",
    "    model.train_xent_loss = []\n",
    "    model.train_temp_hist = []\n",
    "    model.train_temp_grad_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5039e3f1-5348-499e-9323-c1b205f4b5cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:36.699705Z",
     "iopub.status.busy": "2024-04-05T22:41:36.699485Z",
     "iopub.status.idle": "2024-04-05T22:41:36.727079Z",
     "shell.execute_reply": "2024-04-05T22:41:36.726432Z",
     "shell.execute_reply.started": "2024-04-05T22:41:36.699685Z"
    }
   },
   "outputs": [],
   "source": [
    "show_every = 1\n",
    "starting_epoch, epoch , epochs = 0,0,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c67b139-50ac-4b41-851c-3f60379dc840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:37.256263Z",
     "iopub.status.busy": "2024-04-05T22:41:37.255823Z",
     "iopub.status.idle": "2024-04-05T22:41:37.294471Z",
     "shell.execute_reply": "2024-04-05T22:41:37.293942Z",
     "shell.execute_reply.started": "2024-04-05T22:41:37.256223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch, epoch , epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6640bc7a-35a0-4387-9cad-92080154620f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:37.666157Z",
     "iopub.status.busy": "2024-04-05T22:41:37.665948Z",
     "iopub.status.idle": "2024-04-05T22:41:37.694859Z",
     "shell.execute_reply": "2024-04-05T22:41:37.694107Z",
     "shell.execute_reply.started": "2024-04-05T22:41:37.666137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " run epochs 1 to 10 \n"
     ]
    }
   ],
   "source": [
    "print(f\" run epochs {starting_epoch+1} to {epochs} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feaee680-379f-4218-a085-2f8b104ea6a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:38.077792Z",
     "iopub.status.busy": "2024-04-05T22:41:38.077509Z",
     "iopub.status.idle": "2024-04-05T22:41:38.103825Z",
     "shell.execute_reply": "2024-04-05T22:41:38.102969Z",
     "shell.execute_reply.started": "2024-04-05T22:41:38.077772Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.temperature , model.snnl_criterion.temperature \n",
    "# model.temperature = 40.0\n",
    "# model.snnl_criterion.temperature = model.temperature \n",
    "# model.temperature\n",
    "# model.snnl_criterion.temperature \n",
    "# model.optimizer.param_groups[0][\"lr\"]\n",
    "# from torch import linalg as LA\n",
    "# LA.vector_norm(model.temperature.grad)\n",
    "# LA.vector_norm(model.snnl_criterion.temperature.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7663755f-0943-4ff9-8175-4921580130dd",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-04-06T04:59:13.067752Z",
     "iopub.status.busy": "2024-04-06T04:59:13.067344Z",
     "iopub.status.idle": "2024-04-06T05:48:35.845958Z",
     "shell.execute_reply": "2024-04-06T05:48:35.845114Z",
     "shell.execute_reply.started": "2024-04-06T04:59:13.067717Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240406_065913  epoch  101 of  150\n",
      "20240406_070012  epoch  101 of  150  mean loss = 49.278454 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 49.278454 |  temp = 5.235223  |  temp grad = 0.169523 \n",
      "20240406_070111  epoch  102 of  150  mean loss = 49.176104 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 49.176104 |  temp = 4.604747  |  temp grad = 0.208398 \n",
      "20240406_070209  epoch  103 of  150  mean loss = 49.037272 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 49.037272 |  temp = 3.927939  |  temp grad = 0.284722 \n",
      "20240406_070308  epoch  104 of  150  mean loss = 48.841936 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 48.841936 |  temp = 3.182779  |  temp grad = 0.407278 \n",
      "20240406_070406  epoch  105 of  150  mean loss = 48.504949 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 48.504949 |  temp = 2.300226  |  temp grad = 0.709154 \n",
      "20240406_070505  epoch  106 of  150  mean loss = 47.742974 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 47.742974 |  temp = 1.130281  |  temp grad = 1.000000 \n",
      "20240406_070603  epoch  107 of  150  mean loss = 44.167397 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 44.167397 |  temp = 0.157890  |  temp grad = 0.829619 \n",
      "20240406_070702  epoch  108 of  150  mean loss = 39.721133 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 39.721133 |  temp = 0.128192  |  temp grad = 1.000000 \n",
      "20240406_070801  epoch  109 of  150  mean loss = 38.236060 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 38.236060 |  temp = 0.116031  |  temp grad = 1.000000 \n",
      "20240406_070859  epoch  110 of  150  mean loss = 37.272171 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 37.272171 |  temp = 0.109243  |  temp grad = -0.535610 \n",
      "20240406_070957  epoch  111 of  150  mean loss = 36.518523 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 36.518523 |  temp = 0.103492  |  temp grad = -1.000000 \n",
      "batch 150 - layer 0\n",
      "batch 258 - layer 0\n",
      "batch 380 - layer 0\n",
      "batch 389 - layer 0\n",
      "20240406_071056  epoch  112 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.098500  |  temp grad = -1.000000 \n",
      "batch 150 - layer 0\n",
      "batch 235 - layer 0\n",
      "batch 295 - layer 0\n",
      "batch 364 - layer 0\n",
      "20240406_071155  epoch  113 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.094596  |  temp grad = -1.000000 \n",
      "batch 57 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 202 - layer 2\n",
      "batch 235 - layer 0\n",
      "batch 257 - layer 0\n",
      "batch 258 - layer 0\n",
      "batch 259 - layer 0\n",
      "batch 275 - layer 0\n",
      "batch 280 - layer 1\n",
      "batch 369 - layer 0\n",
      "batch 378 - layer 0\n",
      "batch 380 - layer 0\n",
      "20240406_071253  epoch  114 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.091501  |  temp grad = -1.000000 \n",
      "batch 13 - layer 0\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 39 - layer 0\n",
      "batch 71 - layer 0\n",
      "batch 77 - layer 0\n",
      "batch 130 - layer 2\n",
      "batch 144 - layer 0\n",
      "batch 149 - layer 2\n",
      "batch 151 - layer 2\n",
      "batch 196 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 230 - layer 0\n",
      "batch 235 - layer 0\n",
      "batch 244 - layer 0\n",
      "batch 269 - layer 0\n",
      "batch 307 - layer 2\n",
      "batch 315 - layer 0\n",
      "batch 317 - layer 2\n",
      "batch 337 - layer 0\n",
      "batch 360 - layer 0\n",
      "batch 373 - layer 2\n",
      "batch 378 - layer 0\n",
      "batch 380 - layer 0\n",
      "20240406_071352  epoch  115 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.089038  |  temp grad = -1.000000 \n",
      "batch 10 - layer 0\n",
      "batch 11 - layer 0\n",
      "batch 28 - layer 0\n",
      "batch 46 - layer 0\n",
      "batch 57 - layer 2\n",
      "batch 58 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 150 - layer 0\n",
      "batch 200 - layer 0\n",
      "batch 201 - layer 2\n",
      "batch 208 - layer 2\n",
      "batch 252 - layer 2\n",
      "batch 258 - layer 0\n",
      "batch 259 - layer 0\n",
      "batch 291 - layer 0\n",
      "batch 294 - layer 0\n",
      "batch 295 - layer 0\n",
      "batch 295 - layer 2\n",
      "batch 299 - layer 0\n",
      "batch 310 - layer 0\n",
      "batch 312 - layer 1\n",
      "batch 324 - layer 2\n",
      "batch 342 - layer 2\n",
      "batch 343 - layer 0\n",
      "batch 354 - layer 4\n",
      "20240406_071450  epoch  116 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.087720  |  temp grad = -1.000000 \n",
      "batch 11 - layer 0\n",
      "batch 13 - layer 0\n",
      "batch 13 - layer 2\n",
      "batch 30 - layer 0\n",
      "batch 38 - layer 0\n",
      "batch 45 - layer 1\n",
      "batch 46 - layer 0\n",
      "batch 57 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 68 - layer 0\n",
      "batch 71 - layer 0\n",
      "batch 79 - layer 0\n",
      "batch 114 - layer 2\n",
      "batch 123 - layer 1\n",
      "batch 144 - layer 0\n",
      "batch 179 - layer 0\n",
      "batch 183 - layer 0\n",
      "batch 187 - layer 0\n",
      "batch 201 - layer 0\n",
      "batch 201 - layer 2\n",
      "batch 208 - layer 2\n",
      "batch 216 - layer 0\n",
      "batch 217 - layer 0\n",
      "batch 228 - layer 0\n",
      "batch 237 - layer 2\n",
      "batch 294 - layer 0\n",
      "batch 299 - layer 0\n",
      "batch 321 - layer 0\n",
      "batch 331 - layer 0\n",
      "batch 334 - layer 0\n",
      "batch 364 - layer 0\n",
      "batch 382 - layer 0\n",
      "batch 384 - layer 0\n",
      "batch 387 - layer 0\n",
      "batch 388 - layer 0\n",
      "batch 389 - layer 0\n",
      "batch 391 - layer 0\n",
      "batch 391 - layer 1\n",
      "20240406_071549  epoch  117 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.086303  |  temp grad = -1.000000 \n",
      "batch 44 - layer 0\n",
      "batch 48 - layer 0\n",
      "batch 54 - layer 0\n",
      "batch 57 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 65 - layer 1\n",
      "batch 66 - layer 1\n",
      "batch 68 - layer 0\n",
      "batch 76 - layer 0\n",
      "batch 130 - layer 1\n",
      "batch 131 - layer 0\n",
      "batch 134 - layer 0\n",
      "batch 141 - layer 0\n",
      "batch 144 - layer 0\n",
      "batch 155 - layer 0\n",
      "batch 177 - layer 2\n",
      "batch 178 - layer 0\n",
      "batch 181 - layer 0\n",
      "batch 182 - layer 0\n",
      "batch 186 - layer 0\n",
      "batch 189 - layer 0\n",
      "batch 197 - layer 0\n",
      "batch 199 - layer 2\n",
      "batch 201 - layer 0\n",
      "batch 215 - layer 1\n",
      "batch 216 - layer 0\n",
      "batch 220 - layer 0\n",
      "batch 221 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 225 - layer 0\n",
      "batch 228 - layer 0\n",
      "batch 231 - layer 0\n",
      "batch 232 - layer 0\n",
      "batch 234 - layer 0\n",
      "batch 235 - layer 0\n",
      "batch 236 - layer 0\n",
      "batch 237 - layer 0\n",
      "batch 238 - layer 0\n",
      "batch 239 - layer 0\n",
      "batch 239 - layer 1\n",
      "batch 240 - layer 0\n",
      "batch 242 - layer 0\n",
      "batch 269 - layer 0\n",
      "batch 270 - layer 0\n",
      "batch 292 - layer 0\n",
      "batch 294 - layer 0\n",
      "batch 299 - layer 0\n",
      "batch 301 - layer 0\n",
      "batch 304 - layer 0\n",
      "batch 310 - layer 0\n",
      "batch 312 - layer 0\n",
      "batch 327 - layer 0\n",
      "batch 331 - layer 0\n",
      "batch 337 - layer 1\n",
      "batch 338 - layer 0\n",
      "batch 342 - layer 0\n",
      "batch 355 - layer 0\n",
      "batch 378 - layer 0\n",
      "batch 399 - layer 0\n",
      "20240406_071648  epoch  118 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.082703  |  temp grad = 1.000000 \n",
      "batch 3 - layer 0\n",
      "batch 4 - layer 0\n",
      "batch 11 - layer 2\n",
      "batch 20 - layer 1\n",
      "batch 20 - layer 2\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 35 - layer 0\n",
      "batch 35 - layer 1\n",
      "batch 48 - layer 2\n",
      "batch 57 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 61 - layer 0\n",
      "batch 66 - layer 0\n",
      "batch 71 - layer 0\n",
      "batch 73 - layer 0\n",
      "batch 83 - layer 1\n",
      "batch 96 - layer 2\n",
      "batch 117 - layer 2\n",
      "batch 131 - layer 1\n",
      "batch 150 - layer 0\n",
      "batch 152 - layer 0\n",
      "batch 153 - layer 3\n",
      "batch 157 - layer 2\n",
      "batch 221 - layer 0\n",
      "batch 225 - layer 0\n",
      "batch 230 - layer 0\n",
      "batch 233 - layer 0\n",
      "batch 247 - layer 2\n",
      "batch 258 - layer 0\n",
      "batch 268 - layer 3\n",
      "batch 278 - layer 1\n",
      "batch 279 - layer 3\n",
      "batch 282 - layer 0\n",
      "batch 289 - layer 0\n",
      "batch 294 - layer 2\n",
      "batch 300 - layer 0\n",
      "batch 329 - layer 0\n",
      "batch 331 - layer 0\n",
      "batch 333 - layer 0\n",
      "batch 336 - layer 3\n",
      "batch 338 - layer 2\n",
      "batch 341 - layer 0\n",
      "batch 341 - layer 2\n",
      "batch 342 - layer 0\n",
      "batch 343 - layer 0\n",
      "batch 343 - layer 3\n",
      "batch 346 - layer 0\n",
      "batch 346 - layer 1\n",
      "batch 353 - layer 0\n",
      "batch 354 - layer 0\n",
      "batch 357 - layer 0\n",
      "batch 359 - layer 0\n",
      "batch 360 - layer 1\n",
      "batch 360 - layer 2\n",
      "batch 373 - layer 0\n",
      "batch 377 - layer 0\n",
      "batch 382 - layer 2\n",
      "20240406_071746  epoch  119 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.086758  |  temp grad = -1.000000 \n",
      "batch 13 - layer 0\n",
      "batch 17 - layer 0\n",
      "batch 20 - layer 1\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 34 - layer 0\n",
      "batch 38 - layer 0\n",
      "batch 39 - layer 0\n",
      "batch 39 - layer 1\n",
      "batch 42 - layer 0\n",
      "batch 44 - layer 0\n",
      "batch 45 - layer 0\n",
      "batch 46 - layer 0\n",
      "batch 48 - layer 0\n",
      "batch 51 - layer 3\n",
      "batch 52 - layer 0\n",
      "batch 57 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 62 - layer 0\n",
      "batch 63 - layer 0\n",
      "batch 67 - layer 2\n",
      "batch 79 - layer 0\n",
      "batch 81 - layer 3\n",
      "batch 106 - layer 3\n",
      "batch 108 - layer 2\n",
      "batch 113 - layer 2\n",
      "batch 134 - layer 0\n",
      "batch 137 - layer 3\n",
      "batch 144 - layer 0\n",
      "batch 146 - layer 1\n",
      "batch 150 - layer 0\n",
      "batch 152 - layer 0\n",
      "batch 155 - layer 0\n",
      "batch 180 - layer 0\n",
      "batch 182 - layer 0\n",
      "batch 187 - layer 0\n",
      "batch 194 - layer 0\n",
      "batch 197 - layer 0\n",
      "batch 214 - layer 2\n",
      "batch 223 - layer 0\n",
      "batch 228 - layer 0\n",
      "batch 230 - layer 0\n",
      "batch 231 - layer 0\n",
      "batch 234 - layer 0\n",
      "batch 239 - layer 0\n",
      "batch 240 - layer 0\n",
      "batch 241 - layer 0\n",
      "batch 242 - layer 0\n",
      "batch 243 - layer 0\n",
      "batch 244 - layer 0\n",
      "batch 245 - layer 0\n",
      "batch 246 - layer 0\n",
      "batch 248 - layer 0\n",
      "batch 249 - layer 0\n",
      "batch 256 - layer 0\n",
      "batch 257 - layer 0\n",
      "batch 262 - layer 3\n",
      "batch 266 - layer 1\n",
      "batch 278 - layer 3\n",
      "batch 289 - layer 0\n",
      "batch 294 - layer 3\n",
      "batch 299 - layer 0\n",
      "batch 306 - layer 0\n",
      "batch 307 - layer 0\n",
      "batch 308 - layer 2\n",
      "batch 318 - layer 2\n",
      "batch 319 - layer 2\n",
      "batch 325 - layer 2\n",
      "batch 327 - layer 0\n",
      "batch 329 - layer 2\n",
      "batch 331 - layer 0\n",
      "batch 336 - layer 2\n",
      "batch 342 - layer 0\n",
      "batch 342 - layer 2\n",
      "batch 364 - layer 0\n",
      "batch 369 - layer 0\n",
      "batch 378 - layer 0\n",
      "batch 380 - layer 0\n",
      "batch 382 - layer 4\n",
      "batch 387 - layer 0\n",
      "batch 399 - layer 2\n",
      "20240406_071845  epoch  120 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.085273  |  temp grad = -1.000000 \n",
      "batch 4 - layer 0\n",
      "batch 6 - layer 2\n",
      "batch 9 - layer 3\n",
      "batch 13 - layer 0\n",
      "batch 16 - layer 0\n",
      "batch 18 - layer 0\n",
      "batch 20 - layer 0\n",
      "batch 21 - layer 0\n",
      "batch 25 - layer 0\n",
      "batch 28 - layer 0\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 34 - layer 0\n",
      "batch 34 - layer 2\n",
      "batch 44 - layer 4\n",
      "batch 46 - layer 0\n",
      "batch 49 - layer 2\n",
      "batch 60 - layer 0\n",
      "batch 68 - layer 0\n",
      "batch 79 - layer 0\n",
      "batch 80 - layer 0\n",
      "batch 81 - layer 0\n",
      "batch 82 - layer 0\n",
      "batch 82 - layer 3\n",
      "batch 84 - layer 0\n",
      "batch 88 - layer 3\n",
      "batch 92 - layer 2\n",
      "batch 102 - layer 3\n",
      "batch 104 - layer 1\n",
      "batch 106 - layer 0\n",
      "batch 108 - layer 0\n",
      "batch 110 - layer 0\n",
      "batch 111 - layer 0\n",
      "batch 112 - layer 1\n",
      "batch 113 - layer 0\n",
      "batch 113 - layer 2\n",
      "batch 114 - layer 0\n",
      "batch 115 - layer 0\n",
      "batch 115 - layer 1\n",
      "batch 116 - layer 0\n",
      "batch 116 - layer 2\n",
      "batch 117 - layer 0\n",
      "batch 118 - layer 0\n",
      "batch 119 - layer 0\n",
      "batch 120 - layer 0\n",
      "batch 120 - layer 2\n",
      "batch 121 - layer 0\n",
      "batch 123 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 231 - layer 0\n",
      "batch 244 - layer 0\n",
      "batch 251 - layer 3\n",
      "batch 275 - layer 4\n",
      "batch 286 - layer 0\n",
      "batch 292 - layer 3\n",
      "batch 292 - layer 4\n",
      "batch 294 - layer 0\n",
      "batch 295 - layer 0\n",
      "batch 299 - layer 0\n",
      "batch 300 - layer 0\n",
      "batch 320 - layer 0\n",
      "batch 327 - layer 0\n",
      "batch 329 - layer 2\n",
      "batch 332 - layer 1\n",
      "batch 350 - layer 0\n",
      "batch 351 - layer 1\n",
      "batch 358 - layer 1\n",
      "batch 361 - layer 1\n",
      "batch 361 - layer 2\n",
      "batch 365 - layer 2\n",
      "batch 377 - layer 4\n",
      "batch 384 - layer 0\n",
      "batch 384 - layer 4\n",
      "batch 387 - layer 0\n",
      "batch 388 - layer 0\n",
      "batch 389 - layer 0\n",
      "batch 390 - layer 0\n",
      "batch 393 - layer 0\n",
      "batch 394 - layer 1\n",
      "batch 394 - layer 2\n",
      "batch 396 - layer 0\n",
      "batch 397 - layer 0\n",
      "batch 398 - layer 0\n",
      "batch 399 - layer 1\n",
      "20240406_071944  epoch  121 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.078057  |  temp grad = -1.000000 \n",
      "batch 4 - layer 0\n",
      "batch 7 - layer 1\n",
      "batch 8 - layer 2\n",
      "batch 8 - layer 4\n",
      "batch 9 - layer 0\n",
      "batch 11 - layer 0\n",
      "batch 13 - layer 0\n",
      "batch 17 - layer 0\n",
      "batch 29 - layer 4\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 38 - layer 0\n",
      "batch 47 - layer 0\n",
      "batch 48 - layer 0\n",
      "batch 49 - layer 0\n",
      "batch 51 - layer 4\n",
      "batch 52 - layer 0\n",
      "batch 52 - layer 3\n",
      "batch 54 - layer 0\n",
      "batch 55 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 60 - layer 4\n",
      "batch 73 - layer 0\n",
      "batch 77 - layer 0\n",
      "batch 94 - layer 0\n",
      "batch 104 - layer 4\n",
      "batch 124 - layer 1\n",
      "batch 128 - layer 0\n",
      "batch 131 - layer 0\n",
      "batch 132 - layer 0\n",
      "batch 135 - layer 4\n",
      "batch 140 - layer 3\n",
      "batch 146 - layer 4\n",
      "batch 147 - layer 0\n",
      "batch 150 - layer 0\n",
      "batch 151 - layer 4\n",
      "batch 152 - layer 0\n",
      "batch 153 - layer 2\n",
      "batch 153 - layer 3\n",
      "batch 160 - layer 0\n",
      "batch 160 - layer 4\n",
      "batch 168 - layer 4\n",
      "batch 178 - layer 0\n",
      "batch 179 - layer 0\n",
      "batch 192 - layer 3\n",
      "batch 193 - layer 3\n",
      "batch 196 - layer 0\n",
      "batch 199 - layer 4\n",
      "batch 206 - layer 0\n",
      "batch 207 - layer 3\n",
      "batch 210 - layer 2\n",
      "batch 216 - layer 0\n",
      "batch 217 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 225 - layer 0\n",
      "batch 234 - layer 0\n",
      "batch 234 - layer 1\n",
      "batch 234 - layer 2\n",
      "batch 234 - layer 3\n",
      "batch 235 - layer 0\n",
      "batch 240 - layer 1\n",
      "batch 240 - layer 3\n",
      "batch 243 - layer 0\n",
      "batch 254 - layer 3\n",
      "batch 259 - layer 0\n",
      "batch 267 - layer 0\n",
      "batch 272 - layer 2\n",
      "batch 283 - layer 3\n",
      "batch 285 - layer 0\n",
      "batch 289 - layer 0\n",
      "batch 295 - layer 0\n",
      "batch 296 - layer 3\n",
      "batch 301 - layer 0\n",
      "batch 302 - layer 0\n",
      "batch 302 - layer 4\n",
      "batch 306 - layer 0\n",
      "batch 307 - layer 4\n",
      "batch 310 - layer 0\n",
      "batch 313 - layer 2\n",
      "batch 320 - layer 3\n",
      "batch 325 - layer 3\n",
      "batch 329 - layer 4\n",
      "batch 331 - layer 0\n",
      "batch 337 - layer 0\n",
      "batch 337 - layer 1\n",
      "batch 342 - layer 0\n",
      "batch 345 - layer 0\n",
      "batch 348 - layer 2\n",
      "batch 349 - layer 0\n",
      "batch 357 - layer 0\n",
      "batch 360 - layer 0\n",
      "batch 361 - layer 2\n",
      "batch 361 - layer 3\n",
      "batch 363 - layer 1\n",
      "batch 372 - layer 2\n",
      "batch 372 - layer 3\n",
      "batch 372 - layer 4\n",
      "batch 373 - layer 0\n",
      "batch 378 - layer 0\n",
      "batch 380 - layer 0\n",
      "batch 387 - layer 0\n",
      "batch 390 - layer 0\n",
      "batch 391 - layer 0\n",
      "20240406_072042  epoch  122 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.079487  |  temp grad = -1.000000 \n",
      "batch 10 - layer 0\n",
      "batch 17 - layer 0\n",
      "batch 19 - layer 0\n",
      "batch 21 - layer 0\n",
      "batch 22 - layer 0\n",
      "batch 28 - layer 0\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 42 - layer 0\n",
      "batch 44 - layer 0\n",
      "batch 55 - layer 0\n",
      "batch 55 - layer 4\n",
      "batch 58 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 60 - layer 0\n",
      "batch 63 - layer 0\n",
      "batch 68 - layer 0\n",
      "batch 73 - layer 0\n",
      "batch 83 - layer 0\n",
      "batch 89 - layer 3\n",
      "batch 94 - layer 3\n",
      "batch 97 - layer 3\n",
      "batch 101 - layer 2\n",
      "batch 107 - layer 3\n",
      "batch 108 - layer 0\n",
      "batch 109 - layer 2\n",
      "batch 118 - layer 1\n",
      "batch 121 - layer 0\n",
      "batch 123 - layer 0\n",
      "batch 123 - layer 3\n",
      "batch 124 - layer 3\n",
      "batch 139 - layer 0\n",
      "batch 141 - layer 4\n",
      "batch 143 - layer 2\n",
      "batch 144 - layer 0\n",
      "batch 147 - layer 2\n",
      "batch 150 - layer 0\n",
      "batch 152 - layer 0\n",
      "batch 155 - layer 0\n",
      "batch 160 - layer 0\n",
      "batch 161 - layer 3\n",
      "batch 163 - layer 0\n",
      "batch 166 - layer 2\n",
      "batch 166 - layer 3\n",
      "batch 167 - layer 0\n",
      "batch 171 - layer 0\n",
      "batch 174 - layer 0\n",
      "batch 174 - layer 3\n",
      "batch 178 - layer 0\n",
      "batch 182 - layer 1\n",
      "batch 184 - layer 2\n",
      "batch 184 - layer 3\n",
      "batch 186 - layer 0\n",
      "batch 190 - layer 3\n",
      "batch 192 - layer 3\n",
      "batch 196 - layer 0\n",
      "batch 197 - layer 0\n",
      "batch 200 - layer 4\n",
      "batch 202 - layer 3\n",
      "batch 204 - layer 1\n",
      "batch 206 - layer 0\n",
      "batch 208 - layer 0\n",
      "batch 209 - layer 0\n",
      "batch 210 - layer 0\n",
      "batch 211 - layer 0\n",
      "batch 217 - layer 0\n",
      "batch 257 - layer 0\n",
      "batch 259 - layer 0\n",
      "batch 259 - layer 2\n",
      "batch 270 - layer 0\n",
      "batch 275 - layer 0\n",
      "batch 278 - layer 0\n",
      "batch 278 - layer 1\n",
      "batch 282 - layer 0\n",
      "batch 289 - layer 0\n",
      "batch 291 - layer 2\n",
      "batch 299 - layer 0\n",
      "batch 300 - layer 2\n",
      "batch 303 - layer 2\n",
      "batch 303 - layer 4\n",
      "batch 317 - layer 4\n",
      "batch 318 - layer 2\n",
      "batch 327 - layer 0\n",
      "batch 328 - layer 3\n",
      "batch 330 - layer 0\n",
      "batch 330 - layer 3\n",
      "batch 342 - layer 3\n",
      "batch 344 - layer 2\n",
      "batch 348 - layer 2\n",
      "batch 365 - layer 4\n",
      "batch 368 - layer 3\n",
      "batch 372 - layer 1\n",
      "batch 372 - layer 3\n",
      "batch 372 - layer 4\n",
      "batch 380 - layer 0\n",
      "batch 386 - layer 3\n",
      "batch 387 - layer 2\n",
      "batch 388 - layer 3\n",
      "batch 391 - layer 0\n",
      "batch 396 - layer 0\n",
      "batch 398 - layer 3\n",
      "20240406_072141  epoch  123 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.083902  |  temp grad = -1.000000 \n",
      "batch 1 - layer 0\n",
      "batch 3 - layer 0\n",
      "batch 8 - layer 2\n",
      "batch 8 - layer 3\n",
      "batch 8 - layer 4\n",
      "batch 10 - layer 2\n",
      "batch 13 - layer 2\n",
      "batch 13 - layer 4\n",
      "batch 19 - layer 0\n",
      "batch 20 - layer 0\n",
      "batch 21 - layer 0\n",
      "batch 23 - layer 0\n",
      "batch 24 - layer 2\n",
      "batch 24 - layer 3\n",
      "batch 30 - layer 0\n",
      "batch 30 - layer 3\n",
      "batch 30 - layer 4\n",
      "batch 33 - layer 4\n",
      "batch 34 - layer 0\n",
      "batch 38 - layer 2\n",
      "batch 42 - layer 3\n",
      "batch 48 - layer 0\n",
      "batch 50 - layer 3\n",
      "batch 52 - layer 0\n",
      "batch 54 - layer 4\n",
      "batch 58 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 64 - layer 0\n",
      "batch 64 - layer 3\n",
      "batch 68 - layer 0\n",
      "batch 68 - layer 3\n",
      "batch 71 - layer 0\n",
      "batch 72 - layer 0\n",
      "batch 73 - layer 0\n",
      "batch 73 - layer 2\n",
      "batch 76 - layer 0\n",
      "batch 79 - layer 0\n",
      "batch 80 - layer 0\n",
      "batch 80 - layer 2\n",
      "batch 84 - layer 0\n",
      "batch 92 - layer 0\n",
      "batch 100 - layer 2\n",
      "batch 100 - layer 3\n",
      "batch 100 - layer 4\n",
      "batch 111 - layer 3\n",
      "batch 118 - layer 2\n",
      "batch 119 - layer 0\n",
      "batch 120 - layer 0\n",
      "batch 121 - layer 0\n",
      "batch 122 - layer 0\n",
      "batch 123 - layer 2\n",
      "batch 123 - layer 4\n",
      "batch 126 - layer 2\n",
      "batch 126 - layer 3\n",
      "batch 131 - layer 3\n",
      "batch 133 - layer 0\n",
      "batch 134 - layer 4\n",
      "batch 136 - layer 0\n",
      "batch 140 - layer 0\n",
      "batch 141 - layer 0\n",
      "batch 144 - layer 0\n",
      "batch 146 - layer 0\n",
      "batch 146 - layer 3\n",
      "batch 147 - layer 0\n",
      "batch 150 - layer 0\n",
      "batch 150 - layer 2\n",
      "batch 151 - layer 1\n",
      "batch 152 - layer 0\n",
      "batch 153 - layer 2\n",
      "batch 153 - layer 3\n",
      "batch 153 - layer 4\n",
      "batch 161 - layer 0\n",
      "batch 161 - layer 4\n",
      "batch 164 - layer 0\n",
      "batch 170 - layer 0\n",
      "batch 174 - layer 1\n",
      "batch 174 - layer 2\n",
      "batch 174 - layer 3\n",
      "batch 176 - layer 0\n",
      "batch 178 - layer 2\n",
      "batch 181 - layer 0\n",
      "batch 182 - layer 0\n",
      "batch 183 - layer 0\n",
      "batch 184 - layer 0\n",
      "batch 184 - layer 3\n",
      "batch 185 - layer 1\n",
      "batch 186 - layer 4\n",
      "batch 187 - layer 0\n",
      "batch 190 - layer 4\n",
      "batch 196 - layer 0\n",
      "batch 197 - layer 3\n",
      "batch 198 - layer 3\n",
      "batch 201 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 224 - layer 3\n",
      "batch 228 - layer 0\n",
      "batch 228 - layer 4\n",
      "batch 229 - layer 0\n",
      "batch 230 - layer 0\n",
      "batch 230 - layer 2\n",
      "batch 231 - layer 0\n",
      "batch 232 - layer 0\n",
      "batch 233 - layer 0\n",
      "batch 234 - layer 0\n",
      "batch 234 - layer 3\n",
      "batch 235 - layer 0\n",
      "batch 235 - layer 2\n",
      "batch 235 - layer 4\n",
      "batch 236 - layer 0\n",
      "batch 236 - layer 3\n",
      "batch 237 - layer 1\n",
      "batch 237 - layer 4\n",
      "batch 238 - layer 0\n",
      "batch 238 - layer 3\n",
      "batch 239 - layer 0\n",
      "batch 240 - layer 0\n",
      "batch 242 - layer 0\n",
      "batch 243 - layer 0\n",
      "batch 244 - layer 0\n",
      "batch 244 - layer 1\n",
      "batch 245 - layer 1\n",
      "batch 247 - layer 0\n",
      "batch 248 - layer 2\n",
      "batch 248 - layer 3\n",
      "batch 248 - layer 4\n",
      "batch 251 - layer 0\n",
      "batch 252 - layer 3\n",
      "batch 252 - layer 4\n",
      "batch 256 - layer 0\n",
      "batch 257 - layer 0\n",
      "batch 259 - layer 3\n",
      "batch 259 - layer 4\n",
      "batch 260 - layer 2\n",
      "batch 263 - layer 3\n",
      "batch 264 - layer 0\n",
      "batch 264 - layer 3\n",
      "batch 265 - layer 0\n",
      "batch 269 - layer 3\n",
      "batch 271 - layer 3\n",
      "batch 278 - layer 2\n",
      "batch 280 - layer 4\n",
      "batch 281 - layer 3\n",
      "batch 282 - layer 0\n",
      "batch 283 - layer 3\n",
      "batch 284 - layer 0\n",
      "batch 287 - layer 0\n",
      "batch 289 - layer 0\n",
      "batch 290 - layer 2\n",
      "batch 290 - layer 3\n",
      "batch 290 - layer 4\n",
      "batch 291 - layer 3\n",
      "batch 291 - layer 4\n",
      "batch 292 - layer 0\n",
      "batch 296 - layer 3\n",
      "batch 299 - layer 0\n",
      "batch 302 - layer 2\n",
      "batch 303 - layer 2\n",
      "batch 303 - layer 4\n",
      "batch 305 - layer 0\n",
      "batch 306 - layer 0\n",
      "batch 312 - layer 0\n",
      "batch 312 - layer 2\n",
      "batch 323 - layer 4\n",
      "batch 329 - layer 4\n",
      "batch 332 - layer 4\n",
      "batch 334 - layer 0\n",
      "batch 337 - layer 0\n",
      "batch 338 - layer 3\n",
      "batch 338 - layer 4\n",
      "batch 341 - layer 0\n",
      "batch 342 - layer 3\n",
      "batch 344 - layer 3\n",
      "batch 345 - layer 4\n",
      "batch 348 - layer 3\n",
      "batch 352 - layer 3\n",
      "batch 355 - layer 4\n",
      "batch 356 - layer 0\n",
      "batch 356 - layer 4\n",
      "batch 357 - layer 2\n",
      "batch 357 - layer 3\n",
      "batch 357 - layer 4\n",
      "batch 358 - layer 0\n",
      "batch 365 - layer 0\n",
      "batch 368 - layer 0\n",
      "batch 369 - layer 0\n",
      "batch 369 - layer 3\n",
      "batch 373 - layer 0\n",
      "batch 377 - layer 2\n",
      "batch 377 - layer 3\n",
      "batch 377 - layer 4\n",
      "batch 379 - layer 4\n",
      "batch 381 - layer 3\n",
      "batch 384 - layer 0\n",
      "batch 384 - layer 2\n",
      "batch 386 - layer 1\n",
      "batch 386 - layer 3\n",
      "batch 387 - layer 4\n",
      "batch 388 - layer 0\n",
      "batch 392 - layer 2\n",
      "batch 397 - layer 0\n",
      "batch 399 - layer 0\n",
      "20240406_072240  epoch  124 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.076707  |  temp grad = 1.000000 \n",
      "batch 3 - layer 0\n",
      "batch 4 - layer 0\n",
      "batch 5 - layer 0\n",
      "batch 6 - layer 2\n",
      "batch 6 - layer 3\n",
      "batch 7 - layer 2\n",
      "batch 7 - layer 3\n",
      "batch 9 - layer 0\n",
      "batch 10 - layer 0\n",
      "batch 11 - layer 0\n",
      "batch 11 - layer 3\n",
      "batch 12 - layer 1\n",
      "batch 13 - layer 0\n",
      "batch 13 - layer 4\n",
      "batch 15 - layer 0\n",
      "batch 16 - layer 0\n",
      "batch 18 - layer 0\n",
      "batch 18 - layer 4\n",
      "batch 19 - layer 0\n",
      "batch 20 - layer 0\n",
      "batch 21 - layer 0\n",
      "batch 22 - layer 3\n",
      "batch 23 - layer 3\n",
      "batch 25 - layer 0\n",
      "batch 27 - layer 0\n",
      "batch 28 - layer 0\n",
      "batch 30 - layer 0\n",
      "batch 32 - layer 0\n",
      "batch 34 - layer 0\n",
      "batch 35 - layer 0\n",
      "batch 35 - layer 2\n",
      "batch 35 - layer 3\n",
      "batch 38 - layer 4\n",
      "batch 42 - layer 0\n",
      "batch 42 - layer 2\n",
      "batch 42 - layer 3\n",
      "batch 45 - layer 3\n",
      "batch 47 - layer 0\n",
      "batch 50 - layer 4\n",
      "batch 53 - layer 3\n",
      "batch 54 - layer 0\n",
      "batch 55 - layer 0\n",
      "batch 63 - layer 0\n",
      "batch 64 - layer 0\n",
      "batch 64 - layer 3\n",
      "batch 66 - layer 0\n",
      "batch 68 - layer 3\n",
      "batch 70 - layer 0\n",
      "batch 71 - layer 0\n",
      "batch 73 - layer 0\n",
      "batch 76 - layer 4\n",
      "batch 77 - layer 0\n",
      "batch 79 - layer 0\n",
      "batch 81 - layer 0\n",
      "batch 82 - layer 0\n",
      "batch 83 - layer 3\n",
      "batch 84 - layer 0\n",
      "batch 86 - layer 2\n",
      "batch 86 - layer 4\n",
      "batch 87 - layer 0\n",
      "batch 87 - layer 2\n",
      "batch 93 - layer 3\n",
      "batch 96 - layer 0\n",
      "batch 98 - layer 3\n",
      "batch 100 - layer 0\n",
      "batch 100 - layer 4\n",
      "batch 102 - layer 3\n",
      "batch 102 - layer 4\n",
      "batch 106 - layer 0\n",
      "batch 106 - layer 3\n",
      "batch 111 - layer 3\n",
      "batch 120 - layer 2\n",
      "batch 120 - layer 3\n",
      "batch 121 - layer 0\n",
      "batch 122 - layer 0\n",
      "batch 124 - layer 2\n",
      "batch 124 - layer 4\n",
      "batch 125 - layer 0\n",
      "batch 126 - layer 0\n",
      "batch 126 - layer 3\n",
      "batch 127 - layer 0\n",
      "batch 127 - layer 4\n",
      "batch 128 - layer 0\n",
      "batch 129 - layer 1\n",
      "batch 129 - layer 3\n",
      "batch 131 - layer 0\n",
      "batch 132 - layer 0\n",
      "batch 134 - layer 0\n",
      "batch 134 - layer 3\n",
      "batch 135 - layer 0\n",
      "batch 136 - layer 0\n",
      "batch 138 - layer 3\n",
      "batch 138 - layer 4\n",
      "batch 140 - layer 0\n",
      "batch 141 - layer 0\n",
      "batch 141 - layer 2\n",
      "batch 141 - layer 4\n",
      "batch 144 - layer 0\n",
      "batch 147 - layer 0\n",
      "batch 147 - layer 4\n",
      "batch 152 - layer 0\n",
      "batch 153 - layer 3\n",
      "batch 153 - layer 4\n",
      "batch 154 - layer 0\n",
      "batch 154 - layer 2\n",
      "batch 156 - layer 3\n",
      "batch 161 - layer 2\n",
      "batch 161 - layer 3\n",
      "batch 162 - layer 2\n",
      "batch 162 - layer 3\n",
      "batch 164 - layer 0\n",
      "batch 167 - layer 3\n",
      "batch 167 - layer 4\n",
      "batch 170 - layer 0\n",
      "batch 171 - layer 3\n",
      "batch 174 - layer 1\n",
      "batch 174 - layer 3\n",
      "batch 174 - layer 4\n",
      "batch 176 - layer 0\n",
      "batch 176 - layer 3\n",
      "batch 176 - layer 4\n",
      "batch 181 - layer 0\n",
      "batch 182 - layer 3\n",
      "batch 182 - layer 4\n",
      "batch 183 - layer 0\n",
      "batch 184 - layer 0\n",
      "batch 184 - layer 3\n",
      "batch 186 - layer 0\n",
      "batch 186 - layer 3\n",
      "batch 186 - layer 4\n",
      "batch 187 - layer 0\n",
      "batch 192 - layer 0\n",
      "batch 194 - layer 0\n",
      "batch 197 - layer 0\n",
      "batch 202 - layer 4\n",
      "batch 206 - layer 4\n",
      "batch 207 - layer 3\n",
      "batch 208 - layer 4\n",
      "batch 209 - layer 0\n",
      "batch 217 - layer 0\n",
      "batch 217 - layer 4\n",
      "batch 218 - layer 0\n",
      "batch 219 - layer 0\n",
      "batch 221 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 225 - layer 0\n",
      "batch 227 - layer 3\n",
      "batch 227 - layer 4\n",
      "batch 228 - layer 0\n",
      "batch 228 - layer 4\n",
      "batch 229 - layer 0\n",
      "batch 230 - layer 0\n",
      "batch 230 - layer 3\n",
      "batch 232 - layer 2\n",
      "batch 233 - layer 0\n",
      "batch 234 - layer 0\n",
      "batch 235 - layer 0\n",
      "batch 236 - layer 0\n",
      "batch 236 - layer 4\n",
      "batch 237 - layer 4\n",
      "batch 238 - layer 0\n",
      "batch 239 - layer 0\n",
      "batch 241 - layer 0\n",
      "batch 242 - layer 0\n",
      "batch 243 - layer 0\n",
      "batch 244 - layer 0\n",
      "batch 245 - layer 0\n",
      "batch 247 - layer 0\n",
      "batch 247 - layer 3\n",
      "batch 248 - layer 0\n",
      "batch 249 - layer 1\n",
      "batch 250 - layer 0\n",
      "batch 251 - layer 0\n",
      "batch 252 - layer 0\n",
      "batch 252 - layer 3\n",
      "batch 253 - layer 0\n",
      "batch 254 - layer 0\n",
      "batch 255 - layer 0\n",
      "batch 256 - layer 3\n",
      "batch 256 - layer 4\n",
      "batch 257 - layer 0\n",
      "batch 258 - layer 0\n",
      "batch 259 - layer 0\n",
      "batch 263 - layer 0\n",
      "batch 270 - layer 0\n",
      "batch 271 - layer 4\n",
      "batch 275 - layer 0\n",
      "batch 275 - layer 3\n",
      "batch 279 - layer 0\n",
      "batch 280 - layer 0\n",
      "batch 280 - layer 4\n",
      "batch 289 - layer 0\n",
      "batch 290 - layer 0\n",
      "batch 290 - layer 3\n",
      "batch 294 - layer 0\n",
      "batch 297 - layer 4\n",
      "batch 298 - layer 0\n",
      "batch 300 - layer 0\n",
      "batch 301 - layer 0\n",
      "batch 302 - layer 0\n",
      "batch 303 - layer 0\n",
      "batch 303 - layer 4\n",
      "batch 304 - layer 0\n",
      "batch 305 - layer 0\n",
      "batch 306 - layer 0\n",
      "batch 307 - layer 0\n",
      "batch 308 - layer 0\n",
      "batch 309 - layer 0\n",
      "batch 310 - layer 0\n",
      "batch 311 - layer 0\n",
      "batch 312 - layer 0\n",
      "batch 312 - layer 2\n",
      "batch 313 - layer 0\n",
      "batch 313 - layer 3\n",
      "batch 313 - layer 4\n",
      "batch 314 - layer 0\n",
      "batch 315 - layer 0\n",
      "batch 315 - layer 1\n",
      "batch 315 - layer 2\n",
      "batch 315 - layer 3\n",
      "batch 315 - layer 4\n",
      "batch 316 - layer 0\n",
      "batch 317 - layer 0\n",
      "batch 317 - layer 2\n",
      "batch 317 - layer 3\n",
      "batch 317 - layer 4\n",
      "batch 318 - layer 0\n",
      "batch 318 - layer 4\n",
      "batch 319 - layer 0\n",
      "batch 319 - layer 4\n",
      "batch 320 - layer 0\n",
      "batch 320 - layer 1\n",
      "batch 321 - layer 0\n",
      "batch 322 - layer 0\n",
      "batch 322 - layer 2\n",
      "batch 322 - layer 3\n",
      "batch 323 - layer 0\n",
      "batch 323 - layer 1\n",
      "batch 323 - layer 3\n",
      "batch 324 - layer 0\n",
      "batch 324 - layer 2\n",
      "batch 324 - layer 4\n",
      "batch 325 - layer 0\n",
      "batch 326 - layer 0\n",
      "batch 327 - layer 0\n",
      "batch 327 - layer 2\n",
      "batch 327 - layer 4\n",
      "batch 329 - layer 0\n",
      "batch 330 - layer 0\n",
      "batch 331 - layer 4\n",
      "batch 333 - layer 0\n",
      "batch 333 - layer 3\n",
      "batch 334 - layer 0\n",
      "batch 335 - layer 0\n",
      "batch 336 - layer 0\n",
      "batch 337 - layer 0\n",
      "batch 337 - layer 2\n",
      "batch 337 - layer 4\n",
      "batch 338 - layer 2\n",
      "batch 338 - layer 3\n",
      "batch 342 - layer 0\n",
      "batch 350 - layer 2\n",
      "batch 352 - layer 4\n",
      "batch 360 - layer 0\n",
      "batch 363 - layer 0\n",
      "batch 370 - layer 0\n",
      "batch 377 - layer 4\n",
      "batch 378 - layer 0\n",
      "batch 380 - layer 0\n",
      "batch 384 - layer 2\n",
      "batch 384 - layer 3\n",
      "batch 394 - layer 3\n",
      "batch 396 - layer 0\n",
      "batch 397 - layer 0\n",
      "batch 397 - layer 3\n",
      "20240406_072339  epoch  125 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.078983  |  temp grad = -1.000000 \n",
      "batch 0 - layer 0\n",
      "batch 3 - layer 0\n",
      "batch 11 - layer 2\n",
      "batch 12 - layer 0\n",
      "batch 13 - layer 0\n",
      "batch 13 - layer 2\n",
      "batch 16 - layer 0\n",
      "batch 17 - layer 0\n",
      "batch 17 - layer 3\n",
      "batch 19 - layer 0\n",
      "batch 22 - layer 2\n",
      "batch 23 - layer 0\n",
      "batch 28 - layer 0\n",
      "batch 30 - layer 0\n",
      "batch 30 - layer 1\n",
      "batch 32 - layer 3\n",
      "batch 35 - layer 0\n",
      "batch 35 - layer 2\n",
      "batch 35 - layer 3\n",
      "batch 39 - layer 3\n",
      "batch 40 - layer 2\n",
      "batch 42 - layer 0\n",
      "batch 42 - layer 2\n",
      "batch 42 - layer 4\n",
      "batch 44 - layer 0\n",
      "batch 45 - layer 3\n",
      "batch 45 - layer 4\n",
      "batch 46 - layer 0\n",
      "batch 47 - layer 3\n",
      "batch 48 - layer 0\n",
      "batch 48 - layer 4\n",
      "batch 49 - layer 2\n",
      "batch 50 - layer 2\n",
      "batch 52 - layer 3\n",
      "batch 59 - layer 0\n",
      "batch 61 - layer 0\n",
      "batch 62 - layer 4\n",
      "batch 64 - layer 2\n",
      "batch 65 - layer 2\n",
      "batch 71 - layer 0\n",
      "batch 75 - layer 3\n",
      "batch 76 - layer 2\n",
      "batch 76 - layer 4\n",
      "batch 83 - layer 1\n",
      "batch 87 - layer 4\n",
      "batch 94 - layer 0\n",
      "batch 96 - layer 0\n",
      "batch 97 - layer 4\n",
      "batch 98 - layer 4\n",
      "batch 101 - layer 3\n",
      "batch 102 - layer 0\n",
      "batch 103 - layer 0\n",
      "batch 106 - layer 0\n",
      "batch 107 - layer 0\n",
      "batch 107 - layer 3\n",
      "batch 109 - layer 0\n",
      "batch 110 - layer 0\n",
      "batch 111 - layer 0\n",
      "batch 112 - layer 0\n",
      "batch 115 - layer 0\n",
      "batch 115 - layer 2\n",
      "batch 115 - layer 3\n",
      "batch 118 - layer 3\n",
      "batch 119 - layer 0\n",
      "batch 119 - layer 1\n",
      "batch 122 - layer 2\n",
      "batch 122 - layer 3\n",
      "batch 123 - layer 0\n",
      "batch 127 - layer 0\n",
      "batch 128 - layer 3\n",
      "batch 128 - layer 4\n",
      "batch 129 - layer 0\n",
      "batch 131 - layer 0\n",
      "batch 132 - layer 0\n",
      "batch 133 - layer 0\n",
      "batch 133 - layer 2\n",
      "batch 134 - layer 1\n",
      "batch 134 - layer 2\n",
      "batch 134 - layer 4\n",
      "batch 136 - layer 0\n",
      "batch 137 - layer 2\n",
      "batch 137 - layer 3\n",
      "batch 137 - layer 4\n",
      "batch 139 - layer 2\n",
      "batch 139 - layer 4\n",
      "batch 140 - layer 3\n",
      "batch 141 - layer 0\n",
      "batch 141 - layer 2\n",
      "batch 141 - layer 3\n",
      "batch 143 - layer 2\n",
      "batch 144 - layer 0\n",
      "batch 144 - layer 2\n",
      "batch 144 - layer 3\n",
      "batch 145 - layer 0\n",
      "batch 146 - layer 0\n",
      "batch 147 - layer 3\n",
      "batch 147 - layer 4\n",
      "batch 150 - layer 0\n",
      "batch 155 - layer 0\n",
      "batch 155 - layer 3\n",
      "batch 155 - layer 4\n",
      "batch 156 - layer 3\n",
      "batch 161 - layer 2\n",
      "batch 162 - layer 4\n",
      "batch 163 - layer 0\n",
      "batch 163 - layer 2\n",
      "batch 165 - layer 0\n",
      "batch 167 - layer 0\n",
      "batch 174 - layer 0\n",
      "batch 174 - layer 4\n",
      "batch 176 - layer 4\n",
      "batch 179 - layer 0\n",
      "batch 179 - layer 4\n",
      "batch 182 - layer 0\n",
      "batch 182 - layer 2\n",
      "batch 182 - layer 4\n",
      "batch 184 - layer 2\n",
      "batch 186 - layer 0\n",
      "batch 187 - layer 0\n",
      "batch 192 - layer 0\n",
      "batch 194 - layer 0\n",
      "batch 194 - layer 2\n",
      "batch 196 - layer 0\n",
      "batch 196 - layer 4\n",
      "batch 197 - layer 3\n",
      "batch 201 - layer 0\n",
      "batch 201 - layer 2\n",
      "batch 202 - layer 0\n",
      "batch 202 - layer 4\n",
      "batch 205 - layer 0\n",
      "batch 206 - layer 0\n",
      "batch 208 - layer 0\n",
      "batch 208 - layer 4\n",
      "batch 209 - layer 4\n",
      "batch 210 - layer 2\n",
      "batch 210 - layer 3\n",
      "batch 210 - layer 4\n",
      "batch 217 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 225 - layer 0\n",
      "batch 225 - layer 3\n",
      "batch 226 - layer 0\n",
      "batch 227 - layer 0\n",
      "batch 228 - layer 0\n",
      "batch 228 - layer 2\n",
      "batch 230 - layer 0\n",
      "batch 231 - layer 0\n",
      "batch 232 - layer 0\n",
      "batch 233 - layer 0\n",
      "batch 234 - layer 0\n",
      "batch 235 - layer 0\n",
      "batch 236 - layer 0\n",
      "batch 236 - layer 2\n",
      "batch 236 - layer 3\n",
      "batch 237 - layer 0\n",
      "batch 239 - layer 3\n",
      "batch 240 - layer 3\n",
      "batch 241 - layer 0\n",
      "batch 244 - layer 0\n",
      "batch 245 - layer 2\n",
      "batch 257 - layer 1\n",
      "batch 279 - layer 3\n",
      "batch 281 - layer 3\n",
      "batch 281 - layer 4\n",
      "batch 305 - layer 3\n",
      "batch 312 - layer 3\n",
      "batch 312 - layer 4\n",
      "batch 313 - layer 3\n",
      "batch 327 - layer 0\n",
      "batch 337 - layer 0\n",
      "batch 338 - layer 3\n",
      "batch 338 - layer 4\n",
      "batch 342 - layer 2\n",
      "batch 345 - layer 0\n",
      "batch 350 - layer 2\n",
      "batch 350 - layer 3\n",
      "batch 350 - layer 4\n",
      "batch 355 - layer 0\n",
      "batch 357 - layer 2\n",
      "batch 357 - layer 3\n",
      "batch 358 - layer 1\n",
      "batch 370 - layer 3\n",
      "batch 375 - layer 3\n",
      "batch 378 - layer 3\n",
      "batch 378 - layer 4\n",
      "batch 380 - layer 0\n",
      "batch 384 - layer 3\n",
      "batch 388 - layer 2\n",
      "batch 388 - layer 3\n",
      "batch 389 - layer 4\n",
      "20240406_072438  epoch  126 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.081739  |  temp grad = 1.000000 \n",
      "batch 0 - layer 2\n",
      "batch 0 - layer 3\n",
      "batch 0 - layer 4\n",
      "batch 3 - layer 3\n",
      "batch 3 - layer 4\n",
      "batch 6 - layer 3\n",
      "batch 6 - layer 4\n",
      "batch 11 - layer 2\n",
      "batch 13 - layer 0\n",
      "batch 16 - layer 0\n",
      "batch 16 - layer 3\n",
      "batch 17 - layer 4\n",
      "batch 20 - layer 0\n",
      "batch 21 - layer 0\n",
      "batch 22 - layer 0\n",
      "batch 23 - layer 0\n",
      "batch 25 - layer 0\n",
      "batch 27 - layer 0\n",
      "batch 28 - layer 4\n",
      "batch 29 - layer 0\n",
      "batch 30 - layer 0\n",
      "batch 30 - layer 1\n",
      "batch 31 - layer 0\n",
      "batch 32 - layer 0\n",
      "batch 33 - layer 0\n",
      "batch 34 - layer 0\n",
      "batch 34 - layer 3\n",
      "batch 35 - layer 0\n",
      "batch 36 - layer 0\n",
      "batch 40 - layer 2\n",
      "batch 42 - layer 0\n",
      "batch 45 - layer 0\n",
      "batch 46 - layer 0\n",
      "batch 46 - layer 2\n",
      "batch 58 - layer 0\n",
      "batch 76 - layer 4\n",
      "batch 81 - layer 0\n",
      "batch 86 - layer 3\n",
      "batch 100 - layer 2\n",
      "batch 100 - layer 3\n",
      "batch 100 - layer 4\n",
      "batch 101 - layer 0\n",
      "batch 107 - layer 0\n",
      "batch 108 - layer 3\n",
      "batch 108 - layer 4\n",
      "batch 111 - layer 3\n",
      "batch 120 - layer 3\n",
      "batch 120 - layer 4\n",
      "batch 122 - layer 2\n",
      "batch 122 - layer 3\n",
      "batch 127 - layer 0\n",
      "batch 127 - layer 4\n",
      "batch 128 - layer 4\n",
      "batch 134 - layer 0\n",
      "batch 140 - layer 3\n",
      "batch 141 - layer 0\n",
      "batch 141 - layer 3\n",
      "batch 141 - layer 4\n",
      "batch 144 - layer 0\n",
      "batch 145 - layer 3\n",
      "batch 146 - layer 0\n",
      "batch 150 - layer 0\n",
      "batch 154 - layer 4\n",
      "batch 161 - layer 2\n",
      "batch 161 - layer 3\n",
      "batch 161 - layer 4\n",
      "batch 162 - layer 4\n",
      "batch 163 - layer 2\n",
      "batch 165 - layer 0\n",
      "batch 165 - layer 4\n",
      "batch 170 - layer 0\n",
      "batch 171 - layer 2\n",
      "batch 171 - layer 4\n",
      "batch 172 - layer 3\n",
      "batch 178 - layer 0\n",
      "batch 179 - layer 0\n",
      "batch 179 - layer 3\n",
      "batch 180 - layer 2\n",
      "batch 181 - layer 0\n",
      "batch 181 - layer 3\n",
      "batch 184 - layer 4\n",
      "batch 186 - layer 0\n",
      "batch 186 - layer 4\n",
      "batch 192 - layer 0\n",
      "batch 194 - layer 0\n",
      "batch 196 - layer 0\n",
      "batch 197 - layer 0\n",
      "batch 197 - layer 4\n",
      "batch 201 - layer 2\n",
      "batch 202 - layer 3\n",
      "batch 209 - layer 0\n",
      "batch 210 - layer 0\n",
      "batch 216 - layer 0\n",
      "batch 217 - layer 0\n",
      "batch 220 - layer 0\n",
      "batch 221 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 226 - layer 0\n",
      "batch 228 - layer 0\n",
      "batch 230 - layer 3\n",
      "batch 231 - layer 0\n",
      "batch 231 - layer 4\n",
      "batch 232 - layer 0\n",
      "batch 234 - layer 0\n",
      "batch 236 - layer 0\n",
      "batch 236 - layer 2\n",
      "batch 237 - layer 0\n",
      "batch 237 - layer 1\n",
      "batch 237 - layer 4\n",
      "batch 239 - layer 0\n",
      "batch 242 - layer 0\n",
      "batch 256 - layer 0\n",
      "batch 263 - layer 0\n",
      "batch 263 - layer 2\n",
      "batch 263 - layer 3\n",
      "batch 264 - layer 3\n",
      "batch 272 - layer 0\n",
      "batch 272 - layer 4\n",
      "batch 275 - layer 0\n",
      "batch 280 - layer 0\n",
      "batch 281 - layer 1\n",
      "batch 282 - layer 0\n",
      "batch 283 - layer 4\n",
      "batch 289 - layer 0\n",
      "batch 290 - layer 3\n",
      "batch 290 - layer 4\n",
      "batch 292 - layer 0\n",
      "batch 293 - layer 0\n",
      "batch 296 - layer 0\n",
      "batch 297 - layer 0\n",
      "batch 298 - layer 0\n",
      "batch 299 - layer 0\n",
      "batch 300 - layer 0\n",
      "batch 301 - layer 0\n",
      "batch 302 - layer 0\n",
      "batch 303 - layer 0\n",
      "batch 303 - layer 2\n",
      "batch 303 - layer 4\n",
      "batch 304 - layer 0\n",
      "batch 305 - layer 0\n",
      "batch 305 - layer 3\n",
      "batch 305 - layer 4\n",
      "batch 306 - layer 0\n",
      "batch 308 - layer 1\n",
      "batch 308 - layer 4\n",
      "batch 310 - layer 0\n",
      "batch 311 - layer 0\n",
      "batch 312 - layer 0\n",
      "batch 312 - layer 4\n",
      "batch 313 - layer 0\n",
      "batch 315 - layer 0\n",
      "batch 315 - layer 1\n",
      "batch 315 - layer 2\n",
      "batch 315 - layer 4\n",
      "batch 317 - layer 0\n",
      "batch 321 - layer 0\n",
      "batch 326 - layer 3\n",
      "batch 327 - layer 3\n",
      "batch 328 - layer 0\n",
      "batch 330 - layer 3\n",
      "batch 331 - layer 0\n",
      "batch 331 - layer 3\n",
      "batch 334 - layer 0\n",
      "batch 337 - layer 0\n",
      "batch 338 - layer 0\n",
      "batch 341 - layer 0\n",
      "batch 342 - layer 0\n",
      "batch 342 - layer 3\n",
      "batch 342 - layer 4\n",
      "batch 343 - layer 0\n",
      "batch 348 - layer 0\n",
      "batch 349 - layer 0\n",
      "batch 350 - layer 0\n",
      "batch 350 - layer 3\n",
      "batch 355 - layer 0\n",
      "batch 356 - layer 1\n",
      "batch 356 - layer 3\n",
      "batch 361 - layer 0\n",
      "batch 362 - layer 2\n",
      "batch 363 - layer 0\n",
      "batch 363 - layer 3\n",
      "batch 363 - layer 4\n",
      "batch 364 - layer 0\n",
      "batch 364 - layer 1\n",
      "batch 364 - layer 3\n",
      "batch 365 - layer 0\n",
      "batch 366 - layer 0\n",
      "batch 367 - layer 0\n",
      "batch 368 - layer 0\n",
      "batch 369 - layer 0\n",
      "batch 370 - layer 0\n",
      "batch 371 - layer 0\n",
      "batch 372 - layer 0\n",
      "batch 372 - layer 2\n",
      "batch 372 - layer 3\n",
      "batch 373 - layer 0\n",
      "batch 373 - layer 2\n",
      "batch 373 - layer 3\n",
      "batch 374 - layer 0\n",
      "batch 375 - layer 0\n",
      "batch 375 - layer 3\n",
      "batch 376 - layer 0\n",
      "batch 376 - layer 3\n",
      "batch 376 - layer 4\n",
      "batch 377 - layer 0\n",
      "batch 377 - layer 1\n",
      "batch 377 - layer 3\n",
      "batch 378 - layer 0\n",
      "batch 378 - layer 4\n",
      "batch 379 - layer 0\n",
      "batch 379 - layer 3\n",
      "batch 380 - layer 0\n",
      "batch 381 - layer 0\n",
      "batch 381 - layer 3\n",
      "batch 382 - layer 0\n",
      "batch 382 - layer 4\n",
      "batch 383 - layer 0\n",
      "batch 383 - layer 2\n",
      "batch 384 - layer 0\n",
      "batch 384 - layer 2\n",
      "batch 384 - layer 3\n",
      "batch 385 - layer 0\n",
      "batch 386 - layer 0\n",
      "batch 386 - layer 2\n",
      "batch 387 - layer 0\n",
      "batch 387 - layer 3\n",
      "batch 387 - layer 4\n",
      "batch 388 - layer 0\n",
      "batch 389 - layer 0\n",
      "batch 389 - layer 1\n",
      "batch 390 - layer 0\n",
      "batch 390 - layer 1\n",
      "batch 390 - layer 2\n",
      "batch 390 - layer 3\n",
      "batch 391 - layer 0\n",
      "batch 392 - layer 0\n",
      "batch 392 - layer 1\n",
      "batch 393 - layer 0\n",
      "batch 394 - layer 0\n",
      "batch 395 - layer 0\n",
      "batch 396 - layer 0\n",
      "batch 397 - layer 0\n",
      "batch 398 - layer 0\n",
      "batch 399 - layer 0\n",
      "20240406_072537  epoch  127 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.076047  |  temp grad = -1.000000 \n",
      "batch 0 - layer 0\n",
      "batch 1 - layer 0\n",
      "batch 2 - layer 0\n",
      "batch 3 - layer 0\n",
      "batch 4 - layer 0\n",
      "batch 5 - layer 0\n",
      "batch 8 - layer 0\n",
      "batch 10 - layer 0\n",
      "20240406_072636  epoch  128 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.279078  |  temp grad = 0.987057 \n",
      "20240406_072734  epoch  129 of  150  mean loss = 46.690156 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 46.690156 |  temp = 0.259456  |  temp grad = 1.000000 \n",
      "20240406_072833  epoch  130 of  150  mean loss = 46.296099 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 46.296099 |  temp = 0.234616  |  temp grad = 1.000000 \n",
      "20240406_072932  epoch  131 of  150  mean loss = 45.747455 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 45.747455 |  temp = 0.208145  |  temp grad = 0.681785 \n",
      "20240406_073031  epoch  132 of  150  mean loss = 44.999151 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 44.999151 |  temp = 0.183169  |  temp grad = -0.235880 \n",
      "20240406_073132  epoch  133 of  150  mean loss = 44.024958 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 44.024958 |  temp = 0.162814  |  temp grad = -1.000000 \n",
      "20240406_073232  epoch  134 of  150  mean loss = 42.852347 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 42.852348 |  temp = 0.144340  |  temp grad = -1.000000 \n",
      "20240406_073332  epoch  135 of  150  mean loss = 41.580722 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 41.580722 |  temp = 0.130459  |  temp grad = -1.000000 \n",
      "20240406_073431  epoch  136 of  150  mean loss = 40.316812 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 40.316812 |  temp = 0.119769  |  temp grad = -1.000000 \n",
      "20240406_073532  epoch  137 of  150  mean loss = 39.128765 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 39.128765 |  temp = 0.110840  |  temp grad = -1.000000 \n",
      "20240406_073631  epoch  138 of  150  mean loss = 38.042617 |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = 38.042617 |  temp = 0.104127  |  temp grad = -1.000000 \n",
      "batch 44 - layer 0\n",
      "batch 58 - layer 0\n",
      "20240406_073731  epoch  139 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.098541  |  temp grad = -1.000000 \n",
      "batch 16 - layer 0\n",
      "batch 46 - layer 0\n",
      "batch 52 - layer 1\n",
      "batch 186 - layer 0\n",
      "batch 197 - layer 0\n",
      "batch 233 - layer 0\n",
      "batch 345 - layer 0\n",
      "batch 354 - layer 0\n",
      "20240406_073831  epoch  140 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.093354  |  temp grad = -1.000000 \n",
      "batch 23 - layer 0\n",
      "batch 38 - layer 1\n",
      "batch 44 - layer 0\n",
      "batch 54 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 100 - layer 0\n",
      "batch 109 - layer 0\n",
      "batch 177 - layer 0\n",
      "batch 178 - layer 0\n",
      "batch 197 - layer 0\n",
      "batch 233 - layer 1\n",
      "batch 235 - layer 0\n",
      "batch 244 - layer 1\n",
      "batch 276 - layer 0\n",
      "batch 299 - layer 0\n",
      "batch 304 - layer 0\n",
      "batch 360 - layer 0\n",
      "20240406_073931  epoch  141 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.088978  |  temp grad = -1.000000 \n",
      "batch 16 - layer 0\n",
      "batch 23 - layer 1\n",
      "batch 42 - layer 0\n",
      "batch 46 - layer 0\n",
      "batch 52 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 73 - layer 0\n",
      "batch 83 - layer 0\n",
      "batch 87 - layer 0\n",
      "batch 92 - layer 0\n",
      "batch 140 - layer 0\n",
      "batch 144 - layer 0\n",
      "batch 155 - layer 0\n",
      "batch 163 - layer 0\n",
      "batch 168 - layer 2\n",
      "batch 177 - layer 0\n",
      "batch 186 - layer 0\n",
      "batch 187 - layer 2\n",
      "batch 187 - layer 3\n",
      "batch 200 - layer 0\n",
      "batch 203 - layer 0\n",
      "batch 220 - layer 0\n",
      "batch 234 - layer 0\n",
      "batch 254 - layer 1\n",
      "batch 269 - layer 2\n",
      "batch 282 - layer 0\n",
      "batch 285 - layer 0\n",
      "batch 288 - layer 0\n",
      "batch 289 - layer 0\n",
      "batch 299 - layer 0\n",
      "batch 305 - layer 0\n",
      "batch 308 - layer 3\n",
      "batch 321 - layer 0\n",
      "batch 331 - layer 0\n",
      "batch 332 - layer 0\n",
      "batch 333 - layer 0\n",
      "batch 334 - layer 0\n",
      "batch 337 - layer 0\n",
      "batch 341 - layer 1\n",
      "batch 342 - layer 0\n",
      "batch 345 - layer 0\n",
      "batch 348 - layer 1\n",
      "batch 354 - layer 0\n",
      "batch 380 - layer 0\n",
      "batch 395 - layer 0\n",
      "20240406_074032  epoch  142 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.083765  |  temp grad = -1.000000 \n",
      "batch 1 - layer 1\n",
      "batch 10 - layer 0\n",
      "batch 11 - layer 0\n",
      "batch 16 - layer 0\n",
      "batch 30 - layer 0\n",
      "batch 33 - layer 1\n",
      "batch 50 - layer 2\n",
      "batch 54 - layer 0\n",
      "batch 56 - layer 0\n",
      "batch 56 - layer 1\n",
      "batch 57 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 61 - layer 0\n",
      "batch 61 - layer 1\n",
      "batch 77 - layer 2\n",
      "batch 81 - layer 0\n",
      "batch 83 - layer 0\n",
      "batch 88 - layer 0\n",
      "batch 90 - layer 0\n",
      "batch 92 - layer 0\n",
      "batch 98 - layer 1\n",
      "batch 106 - layer 0\n",
      "batch 110 - layer 1\n",
      "batch 136 - layer 2\n",
      "batch 144 - layer 0\n",
      "batch 150 - layer 0\n",
      "batch 155 - layer 1\n",
      "batch 163 - layer 0\n",
      "batch 167 - layer 0\n",
      "batch 168 - layer 0\n",
      "batch 171 - layer 0\n",
      "batch 179 - layer 0\n",
      "batch 183 - layer 0\n",
      "batch 184 - layer 1\n",
      "batch 186 - layer 0\n",
      "batch 187 - layer 2\n",
      "batch 192 - layer 0\n",
      "batch 193 - layer 0\n",
      "batch 193 - layer 1\n",
      "batch 197 - layer 0\n",
      "batch 217 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 231 - layer 0\n",
      "batch 232 - layer 0\n",
      "batch 235 - layer 0\n",
      "batch 237 - layer 0\n",
      "batch 239 - layer 0\n",
      "batch 241 - layer 2\n",
      "batch 245 - layer 1\n",
      "batch 257 - layer 0\n",
      "batch 263 - layer 0\n",
      "batch 278 - layer 1\n",
      "batch 280 - layer 0\n",
      "batch 283 - layer 1\n",
      "batch 285 - layer 1\n",
      "batch 292 - layer 0\n",
      "batch 296 - layer 1\n",
      "batch 298 - layer 0\n",
      "batch 299 - layer 0\n",
      "batch 299 - layer 3\n",
      "batch 307 - layer 4\n",
      "batch 310 - layer 0\n",
      "batch 315 - layer 1\n",
      "batch 316 - layer 1\n",
      "batch 321 - layer 0\n",
      "batch 321 - layer 1\n",
      "batch 331 - layer 0\n",
      "batch 334 - layer 1\n",
      "batch 341 - layer 0\n",
      "batch 347 - layer 3\n",
      "batch 350 - layer 1\n",
      "batch 353 - layer 0\n",
      "batch 354 - layer 4\n",
      "batch 357 - layer 2\n",
      "batch 361 - layer 1\n",
      "batch 369 - layer 0\n",
      "batch 373 - layer 3\n",
      "batch 373 - layer 4\n",
      "batch 374 - layer 2\n",
      "batch 378 - layer 0\n",
      "batch 378 - layer 2\n",
      "batch 383 - layer 1\n",
      "batch 389 - layer 0\n",
      "batch 391 - layer 0\n",
      "batch 394 - layer 1\n",
      "batch 394 - layer 4\n",
      "batch 395 - layer 0\n",
      "20240406_074132  epoch  143 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.084027  |  temp grad = -1.000000 \n",
      "batch 1 - layer 0\n",
      "batch 3 - layer 0\n",
      "batch 8 - layer 0\n",
      "batch 8 - layer 3\n",
      "batch 10 - layer 0\n",
      "batch 10 - layer 4\n",
      "batch 11 - layer 0\n",
      "batch 16 - layer 0\n",
      "batch 17 - layer 0\n",
      "batch 19 - layer 0\n",
      "batch 20 - layer 0\n",
      "batch 20 - layer 3\n",
      "batch 23 - layer 0\n",
      "batch 24 - layer 0\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 33 - layer 1\n",
      "batch 34 - layer 0\n",
      "batch 37 - layer 2\n",
      "batch 41 - layer 0\n",
      "batch 43 - layer 1\n",
      "batch 44 - layer 2\n",
      "batch 44 - layer 3\n",
      "batch 48 - layer 0\n",
      "batch 50 - layer 0\n",
      "batch 52 - layer 4\n",
      "batch 55 - layer 0\n",
      "batch 57 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 63 - layer 1\n",
      "batch 71 - layer 1\n",
      "batch 72 - layer 1\n",
      "batch 76 - layer 0\n",
      "batch 76 - layer 3\n",
      "batch 79 - layer 0\n",
      "batch 79 - layer 4\n",
      "batch 80 - layer 0\n",
      "batch 83 - layer 0\n",
      "batch 83 - layer 1\n",
      "batch 86 - layer 0\n",
      "batch 86 - layer 4\n",
      "batch 87 - layer 0\n",
      "batch 92 - layer 4\n",
      "batch 100 - layer 0\n",
      "batch 109 - layer 0\n",
      "batch 110 - layer 0\n",
      "batch 116 - layer 1\n",
      "batch 126 - layer 2\n",
      "batch 129 - layer 0\n",
      "batch 140 - layer 0\n",
      "batch 144 - layer 0\n",
      "batch 148 - layer 1\n",
      "batch 150 - layer 0\n",
      "batch 151 - layer 0\n",
      "batch 155 - layer 0\n",
      "batch 158 - layer 0\n",
      "batch 161 - layer 0\n",
      "batch 165 - layer 2\n",
      "batch 171 - layer 2\n",
      "batch 179 - layer 3\n",
      "batch 181 - layer 1\n",
      "batch 184 - layer 1\n",
      "batch 186 - layer 0\n",
      "batch 192 - layer 0\n",
      "batch 197 - layer 3\n",
      "batch 197 - layer 4\n",
      "batch 205 - layer 4\n",
      "batch 206 - layer 2\n",
      "batch 208 - layer 2\n",
      "batch 209 - layer 1\n",
      "batch 213 - layer 0\n",
      "batch 213 - layer 1\n",
      "batch 216 - layer 0\n",
      "batch 217 - layer 0\n",
      "batch 220 - layer 1\n",
      "batch 221 - layer 0\n",
      "batch 222 - layer 2\n",
      "batch 223 - layer 0\n",
      "batch 232 - layer 1\n",
      "batch 234 - layer 0\n",
      "batch 243 - layer 3\n",
      "batch 246 - layer 3\n",
      "batch 246 - layer 4\n",
      "batch 253 - layer 0\n",
      "batch 257 - layer 0\n",
      "batch 262 - layer 0\n",
      "batch 264 - layer 0\n",
      "batch 264 - layer 1\n",
      "batch 271 - layer 1\n",
      "batch 271 - layer 4\n",
      "batch 276 - layer 2\n",
      "batch 277 - layer 2\n",
      "batch 280 - layer 0\n",
      "batch 289 - layer 0\n",
      "batch 290 - layer 1\n",
      "batch 292 - layer 3\n",
      "batch 294 - layer 2\n",
      "batch 295 - layer 0\n",
      "batch 298 - layer 0\n",
      "batch 299 - layer 0\n",
      "batch 300 - layer 1\n",
      "batch 301 - layer 0\n",
      "batch 304 - layer 0\n",
      "batch 306 - layer 0\n",
      "batch 309 - layer 0\n",
      "batch 309 - layer 2\n",
      "batch 311 - layer 0\n",
      "batch 312 - layer 0\n",
      "batch 315 - layer 0\n",
      "batch 316 - layer 0\n",
      "batch 316 - layer 1\n",
      "batch 317 - layer 0\n",
      "batch 317 - layer 4\n",
      "batch 318 - layer 0\n",
      "batch 319 - layer 0\n",
      "batch 319 - layer 2\n",
      "batch 320 - layer 0\n",
      "batch 321 - layer 0\n",
      "batch 321 - layer 4\n",
      "batch 322 - layer 0\n",
      "batch 323 - layer 0\n",
      "batch 324 - layer 0\n",
      "batch 329 - layer 0\n",
      "batch 333 - layer 0\n",
      "batch 334 - layer 0\n",
      "batch 336 - layer 1\n",
      "batch 336 - layer 4\n",
      "batch 337 - layer 0\n",
      "batch 341 - layer 0\n",
      "batch 342 - layer 0\n",
      "batch 343 - layer 0\n",
      "batch 344 - layer 2\n",
      "batch 345 - layer 0\n",
      "batch 348 - layer 1\n",
      "batch 348 - layer 3\n",
      "batch 352 - layer 1\n",
      "batch 352 - layer 3\n",
      "batch 353 - layer 1\n",
      "batch 354 - layer 0\n",
      "batch 355 - layer 1\n",
      "batch 356 - layer 1\n",
      "batch 363 - layer 4\n",
      "batch 368 - layer 0\n",
      "batch 369 - layer 0\n",
      "batch 370 - layer 1\n",
      "batch 374 - layer 2\n",
      "batch 378 - layer 0\n",
      "batch 380 - layer 0\n",
      "batch 384 - layer 0\n",
      "batch 390 - layer 1\n",
      "batch 394 - layer 4\n",
      "batch 396 - layer 2\n",
      "batch 397 - layer 2\n",
      "batch 399 - layer 2\n",
      "20240406_074233  epoch  144 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.079969  |  temp grad = -1.000000 \n",
      "batch 0 - layer 0\n",
      "batch 1 - layer 1\n",
      "batch 8 - layer 3\n",
      "batch 9 - layer 1\n",
      "batch 10 - layer 0\n",
      "batch 13 - layer 0\n",
      "batch 16 - layer 0\n",
      "batch 17 - layer 0\n",
      "batch 18 - layer 0\n",
      "batch 20 - layer 0\n",
      "batch 21 - layer 1\n",
      "batch 30 - layer 0\n",
      "batch 30 - layer 3\n",
      "batch 31 - layer 0\n",
      "batch 34 - layer 0\n",
      "batch 35 - layer 0\n",
      "batch 38 - layer 1\n",
      "batch 40 - layer 4\n",
      "batch 42 - layer 0\n",
      "batch 44 - layer 0\n",
      "batch 45 - layer 0\n",
      "batch 46 - layer 0\n",
      "batch 47 - layer 0\n",
      "batch 47 - layer 2\n",
      "batch 48 - layer 0\n",
      "batch 49 - layer 0\n",
      "batch 51 - layer 2\n",
      "batch 54 - layer 0\n",
      "batch 299 - layer 0\n",
      "batch 339 - layer 4\n",
      "batch 354 - layer 0\n",
      "batch 380 - layer 0\n",
      "batch 396 - layer 2\n",
      "batch 397 - layer 0\n",
      "20240406_074334  epoch  145 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.087203  |  temp grad = -1.000000 \n",
      "batch 9 - layer 2\n",
      "batch 10 - layer 4\n",
      "batch 16 - layer 0\n",
      "batch 25 - layer 2\n",
      "batch 38 - layer 3\n",
      "batch 44 - layer 0\n",
      "batch 46 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 68 - layer 0\n",
      "batch 74 - layer 2\n",
      "batch 78 - layer 2\n",
      "batch 80 - layer 1\n",
      "batch 98 - layer 0\n",
      "batch 121 - layer 2\n",
      "batch 127 - layer 0\n",
      "batch 139 - layer 2\n",
      "batch 144 - layer 0\n",
      "batch 144 - layer 1\n",
      "batch 150 - layer 2\n",
      "batch 155 - layer 4\n",
      "batch 165 - layer 1\n",
      "batch 167 - layer 0\n",
      "batch 183 - layer 0\n",
      "batch 192 - layer 2\n",
      "batch 195 - layer 0\n",
      "batch 198 - layer 0\n",
      "batch 201 - layer 0\n",
      "batch 214 - layer 3\n",
      "batch 214 - layer 4\n",
      "batch 220 - layer 0\n",
      "batch 222 - layer 3\n",
      "batch 225 - layer 0\n",
      "batch 229 - layer 0\n",
      "batch 230 - layer 0\n",
      "batch 230 - layer 3\n",
      "batch 234 - layer 0\n",
      "batch 236 - layer 0\n",
      "batch 245 - layer 1\n",
      "batch 249 - layer 3\n",
      "batch 264 - layer 0\n",
      "batch 264 - layer 2\n",
      "batch 265 - layer 2\n",
      "batch 267 - layer 0\n",
      "batch 269 - layer 3\n",
      "batch 269 - layer 4\n",
      "batch 274 - layer 1\n",
      "batch 274 - layer 2\n",
      "batch 278 - layer 0\n",
      "batch 280 - layer 0\n",
      "batch 283 - layer 2\n",
      "batch 283 - layer 3\n",
      "batch 289 - layer 0\n",
      "batch 289 - layer 3\n",
      "batch 289 - layer 4\n",
      "batch 297 - layer 1\n",
      "batch 299 - layer 0\n",
      "batch 301 - layer 1\n",
      "batch 301 - layer 4\n",
      "batch 302 - layer 4\n",
      "batch 303 - layer 1\n",
      "batch 304 - layer 0\n",
      "batch 304 - layer 2\n",
      "batch 305 - layer 2\n",
      "batch 307 - layer 2\n",
      "batch 307 - layer 4\n",
      "batch 308 - layer 3\n",
      "batch 311 - layer 0\n",
      "batch 315 - layer 3\n",
      "batch 327 - layer 0\n",
      "batch 328 - layer 3\n",
      "batch 329 - layer 1\n",
      "batch 331 - layer 0\n",
      "batch 334 - layer 0\n",
      "batch 334 - layer 3\n",
      "batch 337 - layer 0\n",
      "batch 339 - layer 2\n",
      "batch 342 - layer 0\n",
      "batch 342 - layer 1\n",
      "batch 342 - layer 4\n",
      "batch 343 - layer 3\n",
      "batch 344 - layer 2\n",
      "batch 346 - layer 2\n",
      "batch 349 - layer 2\n",
      "batch 350 - layer 4\n",
      "batch 353 - layer 2\n",
      "batch 354 - layer 0\n",
      "batch 356 - layer 2\n",
      "batch 363 - layer 0\n",
      "batch 371 - layer 4\n",
      "batch 374 - layer 2\n",
      "batch 378 - layer 0\n",
      "batch 380 - layer 0\n",
      "batch 391 - layer 0\n",
      "batch 393 - layer 2\n",
      "batch 396 - layer 4\n",
      "batch 399 - layer 0\n",
      "20240406_074435  epoch  146 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.080677  |  temp grad = 1.000000 \n",
      "batch 1 - layer 0\n",
      "batch 3 - layer 0\n",
      "batch 6 - layer 0\n",
      "batch 7 - layer 2\n",
      "batch 8 - layer 1\n",
      "batch 9 - layer 2\n",
      "batch 10 - layer 2\n",
      "batch 11 - layer 0\n",
      "batch 11 - layer 3\n",
      "batch 12 - layer 0\n",
      "batch 13 - layer 4\n",
      "batch 16 - layer 0\n",
      "batch 18 - layer 1\n",
      "batch 24 - layer 1\n",
      "batch 27 - layer 3\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 34 - layer 0\n",
      "batch 38 - layer 0\n",
      "batch 38 - layer 1\n",
      "batch 38 - layer 3\n",
      "batch 38 - layer 4\n",
      "batch 40 - layer 3\n",
      "batch 41 - layer 3\n",
      "batch 42 - layer 0\n",
      "batch 44 - layer 0\n",
      "batch 44 - layer 4\n",
      "batch 47 - layer 2\n",
      "batch 47 - layer 3\n",
      "batch 51 - layer 0\n",
      "batch 52 - layer 0\n",
      "batch 55 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 59 - layer 2\n",
      "batch 60 - layer 3\n",
      "batch 61 - layer 2\n",
      "batch 63 - layer 1\n",
      "batch 68 - layer 0\n",
      "batch 72 - layer 3\n",
      "batch 73 - layer 0\n",
      "batch 75 - layer 0\n",
      "batch 76 - layer 0\n",
      "batch 76 - layer 3\n",
      "batch 76 - layer 4\n",
      "batch 77 - layer 2\n",
      "batch 78 - layer 4\n",
      "batch 80 - layer 2\n",
      "batch 84 - layer 2\n",
      "batch 86 - layer 0\n",
      "batch 88 - layer 1\n",
      "batch 96 - layer 2\n",
      "batch 98 - layer 3\n",
      "batch 98 - layer 4\n",
      "batch 106 - layer 2\n",
      "batch 115 - layer 2\n",
      "batch 117 - layer 2\n",
      "batch 117 - layer 3\n",
      "batch 117 - layer 4\n",
      "batch 123 - layer 2\n",
      "batch 126 - layer 3\n",
      "batch 127 - layer 2\n",
      "batch 129 - layer 0\n",
      "batch 131 - layer 0\n",
      "batch 132 - layer 2\n",
      "batch 134 - layer 2\n",
      "batch 134 - layer 3\n",
      "batch 136 - layer 2\n",
      "batch 139 - layer 4\n",
      "batch 144 - layer 0\n",
      "batch 150 - layer 2\n",
      "batch 151 - layer 0\n",
      "batch 153 - layer 2\n",
      "batch 154 - layer 2\n",
      "batch 157 - layer 0\n",
      "batch 161 - layer 0\n",
      "batch 164 - layer 0\n",
      "batch 167 - layer 0\n",
      "batch 168 - layer 2\n",
      "batch 171 - layer 3\n",
      "batch 175 - layer 1\n",
      "batch 175 - layer 2\n",
      "batch 176 - layer 2\n",
      "batch 178 - layer 0\n",
      "batch 178 - layer 2\n",
      "batch 178 - layer 4\n",
      "batch 180 - layer 2\n",
      "batch 186 - layer 2\n",
      "batch 187 - layer 0\n",
      "batch 192 - layer 2\n",
      "batch 195 - layer 1\n",
      "batch 198 - layer 2\n",
      "batch 198 - layer 4\n",
      "batch 200 - layer 2\n",
      "batch 200 - layer 4\n",
      "batch 201 - layer 0\n",
      "batch 204 - layer 4\n",
      "batch 205 - layer 4\n",
      "batch 212 - layer 2\n",
      "batch 213 - layer 2\n",
      "batch 216 - layer 0\n",
      "batch 216 - layer 1\n",
      "batch 220 - layer 2\n",
      "batch 223 - layer 0\n",
      "batch 223 - layer 2\n",
      "batch 224 - layer 4\n",
      "batch 226 - layer 2\n",
      "batch 227 - layer 2\n",
      "batch 230 - layer 2\n",
      "batch 230 - layer 3\n",
      "batch 230 - layer 4\n",
      "batch 231 - layer 2\n",
      "batch 234 - layer 0\n",
      "batch 236 - layer 2\n",
      "batch 240 - layer 3\n",
      "batch 241 - layer 3\n",
      "batch 244 - layer 0\n",
      "batch 245 - layer 2\n",
      "batch 245 - layer 3\n",
      "batch 247 - layer 3\n",
      "batch 248 - layer 4\n",
      "batch 255 - layer 4\n",
      "batch 256 - layer 2\n",
      "batch 256 - layer 3\n",
      "batch 263 - layer 3\n",
      "batch 263 - layer 4\n",
      "batch 264 - layer 2\n",
      "batch 264 - layer 3\n",
      "batch 264 - layer 4\n",
      "batch 268 - layer 2\n",
      "batch 268 - layer 4\n",
      "batch 270 - layer 0\n",
      "batch 271 - layer 3\n",
      "batch 271 - layer 4\n",
      "batch 272 - layer 2\n",
      "batch 274 - layer 0\n",
      "batch 276 - layer 2\n",
      "batch 278 - layer 0\n",
      "batch 278 - layer 3\n",
      "batch 278 - layer 4\n",
      "batch 282 - layer 0\n",
      "batch 284 - layer 2\n",
      "batch 284 - layer 4\n",
      "batch 285 - layer 2\n",
      "batch 285 - layer 3\n",
      "batch 290 - layer 3\n",
      "batch 291 - layer 2\n",
      "batch 292 - layer 2\n",
      "batch 294 - layer 0\n",
      "batch 296 - layer 1\n",
      "batch 297 - layer 2\n",
      "batch 297 - layer 3\n",
      "batch 297 - layer 4\n",
      "batch 299 - layer 0\n",
      "batch 299 - layer 2\n",
      "batch 304 - layer 0\n",
      "batch 304 - layer 2\n",
      "batch 307 - layer 0\n",
      "batch 308 - layer 0\n",
      "batch 308 - layer 1\n",
      "batch 311 - layer 0\n",
      "batch 311 - layer 3\n",
      "batch 312 - layer 2\n",
      "batch 312 - layer 4\n",
      "batch 315 - layer 0\n",
      "batch 315 - layer 2\n",
      "batch 315 - layer 4\n",
      "batch 316 - layer 2\n",
      "batch 316 - layer 3\n",
      "batch 316 - layer 4\n",
      "batch 317 - layer 2\n",
      "batch 317 - layer 3\n",
      "batch 318 - layer 2\n",
      "batch 319 - layer 4\n",
      "batch 329 - layer 3\n",
      "batch 331 - layer 0\n",
      "batch 331 - layer 2\n",
      "batch 331 - layer 3\n",
      "batch 333 - layer 0\n",
      "batch 337 - layer 0\n",
      "batch 337 - layer 3\n",
      "batch 338 - layer 2\n",
      "batch 338 - layer 3\n",
      "batch 339 - layer 0\n",
      "batch 341 - layer 2\n",
      "batch 343 - layer 2\n",
      "batch 344 - layer 2\n",
      "batch 345 - layer 0\n",
      "batch 345 - layer 2\n",
      "batch 346 - layer 2\n",
      "batch 347 - layer 2\n",
      "batch 348 - layer 2\n",
      "batch 349 - layer 0\n",
      "batch 349 - layer 2\n",
      "batch 350 - layer 4\n",
      "batch 352 - layer 2\n",
      "batch 352 - layer 3\n",
      "batch 352 - layer 4\n",
      "batch 353 - layer 2\n",
      "batch 354 - layer 0\n",
      "batch 355 - layer 0\n",
      "batch 355 - layer 2\n",
      "batch 356 - layer 3\n",
      "batch 356 - layer 4\n",
      "batch 357 - layer 0\n",
      "batch 358 - layer 0\n",
      "batch 359 - layer 0\n",
      "batch 361 - layer 2\n",
      "batch 364 - layer 0\n",
      "batch 365 - layer 2\n",
      "batch 368 - layer 0\n",
      "batch 368 - layer 2\n",
      "batch 371 - layer 3\n",
      "batch 371 - layer 4\n",
      "batch 372 - layer 4\n",
      "batch 373 - layer 2\n",
      "batch 374 - layer 3\n",
      "batch 379 - layer 2\n",
      "batch 379 - layer 4\n",
      "batch 380 - layer 0\n",
      "batch 383 - layer 3\n",
      "batch 384 - layer 2\n",
      "batch 387 - layer 0\n",
      "batch 387 - layer 4\n",
      "batch 388 - layer 4\n",
      "batch 389 - layer 2\n",
      "batch 390 - layer 3\n",
      "batch 394 - layer 3\n",
      "batch 394 - layer 4\n",
      "batch 395 - layer 2\n",
      "batch 395 - layer 3\n",
      "batch 399 - layer 0\n",
      "20240406_074536  epoch  147 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.081833  |  temp grad = 1.000000 \n",
      "batch 1 - layer 0\n",
      "batch 2 - layer 2\n",
      "batch 6 - layer 4\n",
      "batch 7 - layer 2\n",
      "batch 9 - layer 3\n",
      "batch 10 - layer 2\n",
      "batch 11 - layer 0\n",
      "batch 11 - layer 3\n",
      "batch 16 - layer 0\n",
      "batch 16 - layer 4\n",
      "batch 17 - layer 3\n",
      "batch 18 - layer 4\n",
      "batch 19 - layer 2\n",
      "batch 22 - layer 2\n",
      "batch 24 - layer 4\n",
      "batch 27 - layer 2\n",
      "batch 29 - layer 4\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 31 - layer 2\n",
      "batch 31 - layer 4\n",
      "batch 33 - layer 2\n",
      "batch 36 - layer 0\n",
      "batch 36 - layer 4\n",
      "batch 38 - layer 3\n",
      "batch 39 - layer 2\n",
      "batch 41 - layer 0\n",
      "batch 41 - layer 2\n",
      "batch 41 - layer 3\n",
      "batch 42 - layer 0\n",
      "batch 42 - layer 3\n",
      "batch 43 - layer 4\n",
      "batch 44 - layer 2\n",
      "batch 46 - layer 3\n",
      "batch 51 - layer 0\n",
      "batch 51 - layer 2\n",
      "batch 52 - layer 0\n",
      "batch 52 - layer 3\n",
      "batch 54 - layer 0\n",
      "batch 54 - layer 2\n",
      "batch 54 - layer 4\n",
      "batch 55 - layer 0\n",
      "batch 59 - layer 0\n",
      "batch 59 - layer 4\n",
      "batch 60 - layer 3\n",
      "batch 60 - layer 4\n",
      "batch 65 - layer 1\n",
      "batch 68 - layer 0\n",
      "batch 68 - layer 2\n",
      "batch 69 - layer 3\n",
      "batch 74 - layer 2\n",
      "batch 74 - layer 3\n",
      "batch 76 - layer 0\n",
      "batch 76 - layer 4\n",
      "batch 82 - layer 2\n",
      "batch 84 - layer 3\n",
      "batch 85 - layer 3\n",
      "batch 86 - layer 0\n",
      "batch 86 - layer 2\n",
      "batch 87 - layer 1\n",
      "batch 89 - layer 2\n",
      "batch 89 - layer 4\n",
      "batch 90 - layer 0\n",
      "batch 91 - layer 2\n",
      "batch 91 - layer 3\n",
      "batch 94 - layer 3\n",
      "batch 98 - layer 2\n",
      "batch 98 - layer 4\n",
      "batch 100 - layer 3\n",
      "batch 101 - layer 2\n",
      "batch 101 - layer 4\n",
      "batch 102 - layer 3\n",
      "batch 105 - layer 2\n",
      "batch 107 - layer 2\n",
      "batch 107 - layer 3\n",
      "batch 107 - layer 4\n",
      "batch 111 - layer 2\n",
      "batch 115 - layer 2\n",
      "batch 116 - layer 2\n",
      "batch 116 - layer 4\n",
      "batch 118 - layer 2\n",
      "batch 118 - layer 4\n",
      "batch 120 - layer 4\n",
      "batch 122 - layer 3\n",
      "batch 122 - layer 4\n",
      "batch 123 - layer 2\n",
      "batch 124 - layer 3\n",
      "batch 129 - layer 0\n",
      "batch 130 - layer 4\n",
      "batch 131 - layer 0\n",
      "batch 134 - layer 4\n",
      "batch 135 - layer 3\n",
      "batch 136 - layer 3\n",
      "batch 140 - layer 3\n",
      "batch 141 - layer 0\n",
      "batch 141 - layer 3\n",
      "batch 144 - layer 0\n",
      "batch 145 - layer 2\n",
      "batch 145 - layer 3\n",
      "batch 146 - layer 2\n",
      "batch 152 - layer 0\n",
      "batch 153 - layer 0\n",
      "batch 153 - layer 2\n",
      "batch 155 - layer 3\n",
      "batch 156 - layer 3\n",
      "batch 157 - layer 0\n",
      "batch 158 - layer 0\n",
      "batch 159 - layer 0\n",
      "batch 160 - layer 0\n",
      "batch 160 - layer 2\n",
      "batch 160 - layer 3\n",
      "batch 160 - layer 4\n",
      "batch 161 - layer 2\n",
      "batch 161 - layer 4\n",
      "batch 163 - layer 2\n",
      "batch 163 - layer 3\n",
      "batch 165 - layer 1\n",
      "batch 167 - layer 0\n",
      "batch 168 - layer 0\n",
      "batch 168 - layer 3\n",
      "batch 170 - layer 2\n",
      "batch 170 - layer 4\n",
      "batch 172 - layer 4\n",
      "batch 173 - layer 3\n",
      "batch 176 - layer 0\n",
      "batch 178 - layer 0\n",
      "batch 178 - layer 4\n",
      "batch 179 - layer 0\n",
      "batch 180 - layer 0\n",
      "batch 181 - layer 0\n",
      "batch 185 - layer 2\n",
      "batch 185 - layer 3\n",
      "batch 186 - layer 0\n",
      "batch 187 - layer 0\n",
      "batch 188 - layer 1\n",
      "batch 188 - layer 2\n",
      "batch 188 - layer 4\n",
      "batch 190 - layer 2\n",
      "batch 190 - layer 4\n",
      "batch 192 - layer 0\n",
      "batch 193 - layer 3\n",
      "batch 193 - layer 4\n",
      "batch 195 - layer 2\n",
      "batch 195 - layer 4\n",
      "batch 196 - layer 0\n",
      "batch 196 - layer 3\n",
      "batch 197 - layer 2\n",
      "batch 197 - layer 4\n",
      "batch 198 - layer 2\n",
      "batch 198 - layer 3\n",
      "batch 198 - layer 4\n",
      "batch 200 - layer 0\n",
      "batch 200 - layer 3\n",
      "batch 201 - layer 0\n",
      "batch 203 - layer 3\n",
      "batch 203 - layer 4\n",
      "batch 204 - layer 2\n",
      "batch 207 - layer 0\n",
      "batch 209 - layer 2\n",
      "batch 209 - layer 3\n",
      "batch 209 - layer 4\n",
      "batch 212 - layer 2\n",
      "batch 212 - layer 4\n",
      "batch 217 - layer 0\n",
      "batch 217 - layer 3\n",
      "batch 220 - layer 0\n",
      "batch 221 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 224 - layer 2\n",
      "batch 225 - layer 2\n",
      "batch 225 - layer 4\n",
      "batch 226 - layer 2\n",
      "batch 227 - layer 0\n",
      "batch 227 - layer 1\n",
      "batch 228 - layer 0\n",
      "batch 230 - layer 0\n",
      "batch 230 - layer 2\n",
      "batch 230 - layer 3\n",
      "batch 231 - layer 2\n",
      "batch 231 - layer 3\n",
      "batch 233 - layer 3\n",
      "batch 236 - layer 3\n",
      "batch 236 - layer 4\n",
      "batch 237 - layer 4\n",
      "batch 238 - layer 2\n",
      "batch 238 - layer 4\n",
      "batch 239 - layer 0\n",
      "batch 240 - layer 4\n",
      "batch 241 - layer 2\n",
      "batch 242 - layer 2\n",
      "batch 242 - layer 3\n",
      "batch 242 - layer 4\n",
      "batch 244 - layer 0\n",
      "batch 245 - layer 4\n",
      "batch 247 - layer 3\n",
      "batch 247 - layer 4\n",
      "batch 250 - layer 4\n",
      "batch 256 - layer 0\n",
      "batch 256 - layer 2\n",
      "batch 257 - layer 2\n",
      "batch 257 - layer 4\n",
      "batch 258 - layer 2\n",
      "batch 261 - layer 2\n",
      "batch 263 - layer 2\n",
      "batch 264 - layer 2\n",
      "batch 264 - layer 4\n",
      "batch 269 - layer 0\n",
      "batch 269 - layer 3\n",
      "batch 269 - layer 4\n",
      "batch 272 - layer 0\n",
      "batch 273 - layer 0\n",
      "batch 275 - layer 0\n",
      "batch 275 - layer 4\n",
      "batch 278 - layer 4\n",
      "batch 280 - layer 0\n",
      "batch 280 - layer 2\n",
      "batch 280 - layer 3\n",
      "batch 281 - layer 0\n",
      "batch 282 - layer 0\n",
      "batch 282 - layer 2\n",
      "batch 283 - layer 4\n",
      "batch 284 - layer 2\n",
      "batch 285 - layer 0\n",
      "batch 285 - layer 3\n",
      "batch 286 - layer 3\n",
      "batch 287 - layer 0\n",
      "batch 288 - layer 2\n",
      "batch 288 - layer 3\n",
      "batch 289 - layer 0\n",
      "batch 289 - layer 2\n",
      "batch 289 - layer 3\n",
      "batch 289 - layer 4\n",
      "batch 290 - layer 2\n",
      "batch 292 - layer 0\n",
      "batch 292 - layer 2\n",
      "batch 294 - layer 1\n",
      "batch 296 - layer 3\n",
      "batch 297 - layer 0\n",
      "batch 297 - layer 3\n",
      "batch 298 - layer 0\n",
      "batch 298 - layer 2\n",
      "batch 299 - layer 3\n",
      "batch 299 - layer 4\n",
      "batch 304 - layer 3\n",
      "batch 304 - layer 4\n",
      "batch 308 - layer 0\n",
      "batch 308 - layer 1\n",
      "batch 311 - layer 2\n",
      "batch 312 - layer 2\n",
      "batch 312 - layer 3\n",
      "batch 312 - layer 4\n",
      "batch 313 - layer 2\n",
      "batch 316 - layer 2\n",
      "batch 318 - layer 3\n",
      "batch 319 - layer 2\n",
      "batch 320 - layer 2\n",
      "batch 320 - layer 3\n",
      "batch 323 - layer 2\n",
      "batch 324 - layer 4\n",
      "batch 327 - layer 0\n",
      "batch 328 - layer 2\n",
      "batch 328 - layer 3\n",
      "batch 328 - layer 4\n",
      "batch 329 - layer 2\n",
      "batch 330 - layer 2\n",
      "batch 330 - layer 4\n",
      "batch 331 - layer 0\n",
      "batch 333 - layer 0\n",
      "batch 334 - layer 0\n",
      "batch 334 - layer 3\n",
      "batch 335 - layer 2\n",
      "batch 337 - layer 4\n",
      "batch 338 - layer 4\n",
      "batch 341 - layer 3\n",
      "batch 342 - layer 0\n",
      "batch 342 - layer 1\n",
      "batch 342 - layer 2\n",
      "batch 342 - layer 3\n",
      "batch 345 - layer 2\n",
      "batch 347 - layer 2\n",
      "batch 350 - layer 2\n",
      "batch 350 - layer 3\n",
      "batch 351 - layer 3\n",
      "batch 352 - layer 3\n",
      "batch 355 - layer 0\n",
      "batch 356 - layer 4\n",
      "batch 357 - layer 2\n",
      "batch 357 - layer 3\n",
      "batch 357 - layer 4\n",
      "batch 358 - layer 1\n",
      "batch 358 - layer 2\n",
      "batch 360 - layer 0\n",
      "batch 362 - layer 4\n",
      "batch 366 - layer 2\n",
      "batch 366 - layer 4\n",
      "batch 368 - layer 3\n",
      "batch 368 - layer 4\n",
      "batch 369 - layer 2\n",
      "batch 370 - layer 2\n",
      "batch 370 - layer 3\n",
      "batch 377 - layer 2\n",
      "batch 380 - layer 0\n",
      "batch 381 - layer 2\n",
      "batch 381 - layer 4\n",
      "batch 383 - layer 3\n",
      "batch 383 - layer 4\n",
      "batch 384 - layer 3\n",
      "batch 384 - layer 4\n",
      "batch 385 - layer 2\n",
      "batch 387 - layer 0\n",
      "batch 389 - layer 2\n",
      "batch 390 - layer 3\n",
      "batch 390 - layer 4\n",
      "batch 392 - layer 1\n",
      "batch 396 - layer 4\n",
      "batch 399 - layer 3\n",
      "20240406_074635  epoch  148 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.079992  |  temp grad = -1.000000 \n",
      "batch 3 - layer 1\n",
      "batch 3 - layer 4\n",
      "batch 6 - layer 2\n",
      "batch 7 - layer 2\n",
      "batch 7 - layer 4\n",
      "batch 8 - layer 2\n",
      "batch 10 - layer 0\n",
      "batch 11 - layer 0\n",
      "batch 11 - layer 2\n",
      "batch 12 - layer 0\n",
      "batch 14 - layer 3\n",
      "batch 17 - layer 3\n",
      "batch 17 - layer 4\n",
      "batch 18 - layer 2\n",
      "batch 19 - layer 3\n",
      "batch 20 - layer 2\n",
      "batch 20 - layer 3\n",
      "batch 21 - layer 0\n",
      "batch 25 - layer 4\n",
      "batch 27 - layer 0\n",
      "batch 27 - layer 2\n",
      "batch 27 - layer 4\n",
      "batch 28 - layer 2\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 33 - layer 0\n",
      "batch 34 - layer 0\n",
      "batch 34 - layer 2\n",
      "batch 34 - layer 3\n",
      "batch 34 - layer 4\n",
      "batch 37 - layer 2\n",
      "batch 37 - layer 3\n",
      "batch 38 - layer 3\n",
      "batch 42 - layer 0\n",
      "batch 42 - layer 4\n",
      "batch 43 - layer 2\n",
      "batch 43 - layer 4\n",
      "batch 44 - layer 2\n",
      "batch 52 - layer 2\n",
      "batch 54 - layer 0\n",
      "batch 55 - layer 3\n",
      "batch 57 - layer 0\n",
      "batch 58 - layer 0\n",
      "batch 58 - layer 3\n",
      "batch 60 - layer 3\n",
      "batch 61 - layer 0\n",
      "batch 63 - layer 0\n",
      "batch 64 - layer 0\n",
      "batch 64 - layer 3\n",
      "batch 65 - layer 1\n",
      "batch 65 - layer 4\n",
      "batch 69 - layer 4\n",
      "batch 70 - layer 4\n",
      "batch 73 - layer 0\n",
      "batch 76 - layer 0\n",
      "batch 77 - layer 0\n",
      "batch 77 - layer 4\n",
      "batch 80 - layer 0\n",
      "batch 80 - layer 4\n",
      "batch 81 - layer 0\n",
      "batch 81 - layer 4\n",
      "batch 82 - layer 0\n",
      "batch 82 - layer 3\n",
      "batch 82 - layer 4\n",
      "batch 84 - layer 0\n",
      "batch 85 - layer 2\n",
      "batch 85 - layer 3\n",
      "batch 85 - layer 4\n",
      "batch 90 - layer 0\n",
      "batch 91 - layer 2\n",
      "batch 91 - layer 3\n",
      "batch 92 - layer 0\n",
      "batch 93 - layer 1\n",
      "batch 95 - layer 0\n",
      "batch 96 - layer 2\n",
      "batch 100 - layer 2\n",
      "batch 101 - layer 4\n",
      "batch 102 - layer 2\n",
      "batch 102 - layer 4\n",
      "batch 103 - layer 4\n",
      "batch 105 - layer 0\n",
      "batch 105 - layer 2\n",
      "batch 105 - layer 3\n",
      "batch 106 - layer 0\n",
      "batch 106 - layer 3\n",
      "batch 107 - layer 2\n",
      "batch 107 - layer 4\n",
      "batch 109 - layer 0\n",
      "batch 110 - layer 3\n",
      "batch 111 - layer 0\n",
      "batch 111 - layer 2\n",
      "batch 111 - layer 3\n",
      "batch 112 - layer 4\n",
      "batch 114 - layer 0\n",
      "batch 115 - layer 3\n",
      "batch 115 - layer 4\n",
      "batch 116 - layer 3\n",
      "batch 118 - layer 2\n",
      "batch 118 - layer 3\n",
      "batch 118 - layer 4\n",
      "batch 119 - layer 3\n",
      "batch 122 - layer 2\n",
      "batch 122 - layer 4\n",
      "batch 123 - layer 0\n",
      "batch 124 - layer 3\n",
      "batch 133 - layer 0\n",
      "batch 134 - layer 4\n",
      "batch 135 - layer 0\n",
      "batch 140 - layer 0\n",
      "batch 140 - layer 3\n",
      "batch 141 - layer 2\n",
      "batch 144 - layer 0\n",
      "batch 146 - layer 1\n",
      "batch 146 - layer 3\n",
      "batch 148 - layer 1\n",
      "batch 150 - layer 0\n",
      "batch 155 - layer 2\n",
      "batch 155 - layer 4\n",
      "batch 156 - layer 4\n",
      "batch 157 - layer 0\n",
      "batch 157 - layer 4\n",
      "batch 161 - layer 2\n",
      "batch 161 - layer 4\n",
      "batch 163 - layer 1\n",
      "batch 163 - layer 4\n",
      "batch 168 - layer 4\n",
      "batch 171 - layer 2\n",
      "batch 172 - layer 4\n",
      "batch 173 - layer 3\n",
      "batch 174 - layer 0\n",
      "batch 174 - layer 2\n",
      "batch 174 - layer 3\n",
      "batch 174 - layer 4\n",
      "batch 176 - layer 0\n",
      "batch 181 - layer 2\n",
      "batch 182 - layer 0\n",
      "batch 185 - layer 0\n",
      "batch 185 - layer 2\n",
      "batch 185 - layer 3\n",
      "batch 186 - layer 0\n",
      "batch 186 - layer 2\n",
      "batch 186 - layer 3\n",
      "batch 187 - layer 0\n",
      "batch 187 - layer 3\n",
      "batch 188 - layer 0\n",
      "batch 189 - layer 0\n",
      "batch 189 - layer 3\n",
      "batch 189 - layer 4\n",
      "batch 190 - layer 0\n",
      "batch 191 - layer 0\n",
      "batch 191 - layer 2\n",
      "batch 191 - layer 4\n",
      "batch 192 - layer 0\n",
      "batch 192 - layer 1\n",
      "batch 192 - layer 2\n",
      "batch 192 - layer 3\n",
      "batch 193 - layer 0\n",
      "batch 195 - layer 0\n",
      "batch 196 - layer 2\n",
      "batch 198 - layer 0\n",
      "batch 200 - layer 3\n",
      "batch 201 - layer 3\n",
      "batch 204 - layer 0\n",
      "batch 204 - layer 4\n",
      "batch 205 - layer 2\n",
      "batch 205 - layer 4\n",
      "batch 214 - layer 3\n",
      "batch 217 - layer 0\n",
      "batch 219 - layer 1\n",
      "batch 229 - layer 2\n",
      "batch 235 - layer 0\n",
      "batch 237 - layer 3\n",
      "batch 243 - layer 0\n",
      "batch 246 - layer 3\n",
      "batch 248 - layer 4\n",
      "batch 250 - layer 2\n",
      "batch 252 - layer 3\n",
      "batch 259 - layer 2\n",
      "batch 259 - layer 3\n",
      "batch 263 - layer 1\n",
      "batch 267 - layer 4\n",
      "batch 268 - layer 4\n",
      "batch 269 - layer 0\n",
      "batch 272 - layer 2\n",
      "batch 272 - layer 3\n",
      "batch 278 - layer 4\n",
      "batch 284 - layer 1\n",
      "batch 284 - layer 2\n",
      "batch 285 - layer 2\n",
      "batch 288 - layer 3\n",
      "batch 288 - layer 4\n",
      "batch 292 - layer 0\n",
      "batch 294 - layer 2\n",
      "batch 298 - layer 0\n",
      "batch 299 - layer 2\n",
      "batch 301 - layer 0\n",
      "batch 303 - layer 3\n",
      "batch 304 - layer 0\n",
      "batch 305 - layer 2\n",
      "batch 305 - layer 3\n",
      "batch 307 - layer 2\n",
      "batch 307 - layer 3\n",
      "batch 309 - layer 0\n",
      "batch 309 - layer 2\n",
      "batch 310 - layer 2\n",
      "batch 312 - layer 2\n",
      "batch 313 - layer 2\n",
      "batch 315 - layer 2\n",
      "batch 315 - layer 3\n",
      "batch 317 - layer 0\n",
      "batch 320 - layer 4\n",
      "batch 321 - layer 2\n",
      "batch 322 - layer 3\n",
      "batch 322 - layer 4\n",
      "batch 327 - layer 0\n",
      "batch 328 - layer 2\n",
      "batch 328 - layer 3\n",
      "batch 329 - layer 0\n",
      "batch 331 - layer 0\n",
      "batch 331 - layer 1\n",
      "batch 331 - layer 3\n",
      "batch 334 - layer 0\n",
      "batch 334 - layer 1\n",
      "batch 336 - layer 3\n",
      "batch 338 - layer 2\n",
      "batch 338 - layer 3\n",
      "batch 341 - layer 2\n",
      "batch 342 - layer 2\n",
      "batch 342 - layer 3\n",
      "batch 344 - layer 2\n",
      "batch 345 - layer 3\n",
      "batch 347 - layer 3\n",
      "batch 350 - layer 2\n",
      "batch 351 - layer 2\n",
      "batch 353 - layer 4\n",
      "batch 360 - layer 2\n",
      "batch 361 - layer 2\n",
      "batch 362 - layer 3\n",
      "batch 363 - layer 2\n",
      "batch 364 - layer 3\n",
      "batch 364 - layer 4\n",
      "batch 380 - layer 0\n",
      "batch 382 - layer 2\n",
      "batch 382 - layer 4\n",
      "batch 383 - layer 4\n",
      "batch 387 - layer 0\n",
      "batch 388 - layer 0\n",
      "batch 388 - layer 4\n",
      "batch 389 - layer 0\n",
      "batch 389 - layer 3\n",
      "batch 390 - layer 2\n",
      "batch 390 - layer 4\n",
      "batch 392 - layer 3\n",
      "batch 396 - layer 0\n",
      "batch 399 - layer 4\n",
      "20240406_074735  epoch  149 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.078566  |  temp grad = -1.000000 \n",
      "batch 1 - layer 0\n",
      "batch 2 - layer 2\n",
      "batch 3 - layer 0\n",
      "batch 3 - layer 4\n",
      "batch 4 - layer 0\n",
      "batch 5 - layer 3\n",
      "batch 5 - layer 4\n",
      "batch 6 - layer 2\n",
      "batch 6 - layer 3\n",
      "batch 9 - layer 4\n",
      "batch 10 - layer 2\n",
      "batch 10 - layer 3\n",
      "batch 12 - layer 0\n",
      "batch 12 - layer 2\n",
      "batch 14 - layer 3\n",
      "batch 15 - layer 2\n",
      "batch 15 - layer 4\n",
      "batch 16 - layer 0\n",
      "batch 20 - layer 0\n",
      "batch 20 - layer 3\n",
      "batch 23 - layer 4\n",
      "batch 30 - layer 0\n",
      "batch 31 - layer 0\n",
      "batch 34 - layer 3\n",
      "batch 35 - layer 2\n",
      "batch 38 - layer 0\n",
      "batch 38 - layer 2\n",
      "batch 39 - layer 0\n",
      "batch 41 - layer 0\n",
      "batch 42 - layer 0\n",
      "batch 43 - layer 0\n",
      "batch 43 - layer 3\n",
      "batch 44 - layer 0\n",
      "batch 45 - layer 0\n",
      "batch 46 - layer 0\n",
      "batch 46 - layer 4\n",
      "batch 47 - layer 0\n",
      "batch 48 - layer 0\n",
      "batch 49 - layer 0\n",
      "batch 50 - layer 0\n",
      "batch 51 - layer 0\n",
      "batch 51 - layer 2\n",
      "batch 51 - layer 3\n",
      "batch 51 - layer 4\n",
      "batch 52 - layer 0\n",
      "batch 52 - layer 3\n",
      "batch 52 - layer 4\n",
      "batch 55 - layer 4\n",
      "batch 57 - layer 2\n",
      "batch 58 - layer 2\n",
      "batch 59 - layer 0\n",
      "batch 59 - layer 3\n",
      "batch 60 - layer 0\n",
      "batch 60 - layer 2\n",
      "batch 63 - layer 0\n",
      "batch 65 - layer 2\n",
      "batch 65 - layer 3\n",
      "batch 66 - layer 2\n",
      "batch 68 - layer 0\n",
      "batch 69 - layer 0\n",
      "batch 69 - layer 3\n",
      "batch 72 - layer 0\n",
      "batch 72 - layer 2\n",
      "batch 75 - layer 4\n",
      "batch 76 - layer 3\n",
      "batch 80 - layer 2\n",
      "batch 80 - layer 3\n",
      "batch 81 - layer 0\n",
      "batch 82 - layer 3\n",
      "batch 82 - layer 4\n",
      "batch 83 - layer 2\n",
      "batch 88 - layer 2\n",
      "batch 100 - layer 4\n",
      "batch 101 - layer 2\n",
      "batch 101 - layer 3\n",
      "batch 101 - layer 4\n",
      "batch 102 - layer 2\n",
      "batch 103 - layer 0\n",
      "batch 105 - layer 2\n",
      "batch 106 - layer 2\n",
      "batch 107 - layer 2\n",
      "batch 115 - layer 3\n",
      "batch 115 - layer 4\n",
      "batch 116 - layer 2\n",
      "batch 120 - layer 4\n",
      "batch 122 - layer 4\n",
      "batch 123 - layer 1\n",
      "batch 123 - layer 3\n",
      "batch 133 - layer 0\n",
      "batch 134 - layer 0\n",
      "batch 134 - layer 2\n",
      "batch 135 - layer 3\n",
      "batch 139 - layer 3\n",
      "batch 140 - layer 0\n",
      "batch 140 - layer 2\n",
      "batch 140 - layer 3\n",
      "batch 141 - layer 0\n",
      "batch 141 - layer 2\n",
      "batch 144 - layer 0\n",
      "batch 144 - layer 2\n",
      "batch 146 - layer 2\n",
      "batch 146 - layer 3\n",
      "batch 146 - layer 4\n",
      "batch 150 - layer 2\n",
      "batch 152 - layer 0\n",
      "batch 153 - layer 2\n",
      "batch 155 - layer 0\n",
      "batch 156 - layer 2\n",
      "batch 156 - layer 3\n",
      "batch 156 - layer 4\n",
      "batch 161 - layer 2\n",
      "batch 163 - layer 0\n",
      "batch 163 - layer 3\n",
      "batch 168 - layer 2\n",
      "batch 171 - layer 0\n",
      "batch 174 - layer 0\n",
      "batch 174 - layer 2\n",
      "batch 180 - layer 4\n",
      "batch 182 - layer 0\n",
      "batch 182 - layer 2\n",
      "batch 182 - layer 4\n",
      "batch 183 - layer 0\n",
      "batch 184 - layer 0\n",
      "batch 184 - layer 3\n",
      "batch 186 - layer 3\n",
      "batch 186 - layer 4\n",
      "batch 190 - layer 4\n",
      "batch 193 - layer 4\n",
      "batch 194 - layer 4\n",
      "batch 195 - layer 4\n",
      "batch 196 - layer 4\n",
      "batch 197 - layer 3\n",
      "batch 199 - layer 3\n",
      "batch 199 - layer 4\n",
      "batch 200 - layer 3\n",
      "batch 200 - layer 4\n",
      "batch 201 - layer 0\n",
      "batch 203 - layer 2\n",
      "batch 203 - layer 4\n",
      "batch 208 - layer 2\n",
      "batch 208 - layer 4\n",
      "batch 210 - layer 4\n",
      "batch 217 - layer 0\n",
      "batch 217 - layer 1\n",
      "batch 220 - layer 0\n",
      "batch 223 - layer 0\n",
      "batch 227 - layer 0\n",
      "batch 230 - layer 0\n",
      "batch 230 - layer 2\n",
      "batch 230 - layer 4\n",
      "batch 234 - layer 0\n",
      "batch 235 - layer 0\n",
      "batch 235 - layer 4\n",
      "batch 236 - layer 2\n",
      "batch 237 - layer 4\n",
      "batch 238 - layer 4\n",
      "batch 245 - layer 4\n",
      "batch 246 - layer 0\n",
      "batch 246 - layer 4\n",
      "batch 247 - layer 3\n",
      "batch 249 - layer 2\n",
      "batch 252 - layer 3\n",
      "batch 264 - layer 1\n",
      "batch 264 - layer 4\n",
      "batch 265 - layer 0\n",
      "batch 266 - layer 3\n",
      "batch 266 - layer 4\n",
      "batch 268 - layer 2\n",
      "batch 269 - layer 0\n",
      "batch 274 - layer 4\n",
      "batch 275 - layer 0\n",
      "batch 275 - layer 2\n",
      "batch 278 - layer 0\n",
      "batch 278 - layer 2\n",
      "batch 280 - layer 4\n",
      "batch 283 - layer 3\n",
      "batch 284 - layer 0\n",
      "batch 285 - layer 0\n",
      "batch 287 - layer 0\n",
      "batch 289 - layer 0\n",
      "batch 289 - layer 1\n",
      "batch 291 - layer 4\n",
      "batch 292 - layer 0\n",
      "batch 293 - layer 2\n",
      "batch 294 - layer 0\n",
      "batch 295 - layer 0\n",
      "batch 296 - layer 0\n",
      "batch 296 - layer 2\n",
      "batch 296 - layer 3\n",
      "batch 297 - layer 0\n",
      "batch 297 - layer 2\n",
      "batch 297 - layer 4\n",
      "batch 299 - layer 0\n",
      "batch 300 - layer 0\n",
      "batch 301 - layer 0\n",
      "batch 302 - layer 0\n",
      "batch 303 - layer 2\n",
      "batch 304 - layer 2\n",
      "batch 304 - layer 3\n",
      "batch 305 - layer 0\n",
      "batch 305 - layer 4\n",
      "batch 307 - layer 2\n",
      "batch 312 - layer 4\n",
      "batch 316 - layer 4\n",
      "batch 318 - layer 3\n",
      "batch 320 - layer 3\n",
      "batch 327 - layer 2\n",
      "batch 327 - layer 3\n",
      "batch 328 - layer 3\n",
      "batch 330 - layer 2\n",
      "batch 331 - layer 0\n",
      "batch 333 - layer 0\n",
      "batch 333 - layer 2\n",
      "batch 334 - layer 0\n",
      "batch 335 - layer 3\n",
      "batch 336 - layer 2\n",
      "batch 337 - layer 0\n",
      "batch 337 - layer 2\n",
      "batch 338 - layer 2\n",
      "batch 338 - layer 4\n",
      "batch 342 - layer 2\n",
      "batch 342 - layer 4\n",
      "batch 343 - layer 0\n",
      "batch 346 - layer 0\n",
      "batch 348 - layer 4\n",
      "batch 349 - layer 0\n",
      "batch 350 - layer 4\n",
      "batch 354 - layer 0\n",
      "batch 355 - layer 0\n",
      "batch 355 - layer 1\n",
      "batch 358 - layer 3\n",
      "batch 359 - layer 0\n",
      "batch 360 - layer 4\n",
      "batch 362 - layer 2\n",
      "batch 364 - layer 2\n",
      "batch 366 - layer 2\n",
      "batch 366 - layer 3\n",
      "batch 373 - layer 0\n",
      "batch 376 - layer 0\n",
      "batch 377 - layer 4\n",
      "batch 380 - layer 0\n",
      "batch 381 - layer 4\n",
      "batch 382 - layer 4\n",
      "batch 383 - layer 4\n",
      "batch 384 - layer 0\n",
      "batch 384 - layer 2\n",
      "batch 384 - layer 3\n",
      "batch 387 - layer 0\n",
      "batch 389 - layer 0\n",
      "batch 389 - layer 3\n",
      "batch 389 - layer 4\n",
      "batch 391 - layer 0\n",
      "batch 392 - layer 1\n",
      "batch 392 - layer 2\n",
      "batch 395 - layer 0\n",
      "batch 399 - layer 0\n",
      "batch 399 - layer 2\n",
      "20240406_074835  epoch  150 of  150  mean loss = nan |  mean acc = 0.000000 |  xent loss = 0.000000 |  snn loss = nan |  temp = 0.077886  |  temp grad = -1.000000 \n"
     ]
    }
   ],
   "source": [
    "print(f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}  epoch {starting_epoch+1:4d} of {epochs:4d}\")\n",
    "\n",
    "for epoch in range(starting_epoch,epochs):\n",
    "    \n",
    "    fit_epoch_loss, fit_epoch_accuracy = model.epoch_train( train_loader, epoch, factor = 10, verbose = False)\n",
    "\n",
    "    model.train_loss.append(fit_epoch_loss[0])\n",
    "    model.train_snn_loss.append(fit_epoch_loss[1])\n",
    "    model.train_xent_loss.append(fit_epoch_loss[2])\n",
    "    model.train_accuracy.append(fit_epoch_accuracy)\n",
    "    model.train_temp_hist.append(model.temperature.item())\n",
    "    model.train_temp_grad_hist.append(model.temperature.grad.item())\n",
    "\n",
    "    if (epoch + 1) % show_every == 0:\n",
    "        print(f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}  epoch {epoch + 1:4d} of {epochs:4d}\"\n",
    "              f\"  mean loss = {model.train_loss[-1]:.6f} |  mean acc = {model.train_accuracy[-1]:.6f} |\"\n",
    "              f\"  xent loss = {model.train_xent_loss[-1]:.6f} |  snn loss = {model.train_snn_loss[-1]:.6f} |\"\n",
    "              f\"  temp = {model.temperature.item():.6f}  |  temp grad = {model.temperature.grad.item():.6f} \")\n",
    "\n",
    "    # model.scheduler.step(model.train_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4680fb7f-3f17-4af7-8844-ef7958de441f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T23:22:02.313197Z",
     "iopub.status.busy": "2024-04-05T23:22:02.312675Z",
     "iopub.status.idle": "2024-04-05T23:22:02.361942Z",
     "shell.execute_reply": "2024-04-05T23:22:02.361281Z",
     "shell.execute_reply.started": "2024-04-05T23:22:02.313152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 25, 24)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch, epochs, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55b50be1-048b-4b10-8532-ee467891c006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T04:56:59.661653Z",
     "iopub.status.busy": "2024-04-06T04:56:59.661183Z",
     "iopub.status.idle": "2024-04-06T04:56:59.709772Z",
     "shell.execute_reply": "2024-04-06T04:56:59.709061Z",
     "shell.execute_reply.started": "2024-04-06T04:56:59.661611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " run epochs 101 to 150  -- 99\n"
     ]
    }
   ],
   "source": [
    "# epochs = 80 \n",
    "epochs = epochs + 50\n",
    "starting_epoch = epoch + 1\n",
    "\n",
    "print(f\" run epochs {starting_epoch+1} to {epochs}  -- {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913fe5e-fbfe-435e-aa02-dbdec688df65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### SNNL Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3926ff5-b3ed-4eea-bcb6-746640817bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.snnl_criterion.layers_snnl\n",
    "# model.snnl_criterion.snnl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f2cf1-ed28-47ad-aacc-ab9a4ac0661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.snnl_criterion.pairwise_distance_matrix[:9,:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c5575-cf98-433c-bcb5-6be9720125fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.snnl_criterion.pick_probability[:9,:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc050b2d-b5de-4b02-92be-8131ace2312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.snnl_criterion.masked_pick_probability[:9,:9]\n",
    "9.4345e-04+  6.9088e-04+  2.6218e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24f44e-0cf6-4879-ba34-b38af40df10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.snnl_criterion.summed_masked_pick_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421057d-38c5-46d3-b601-ce82ed95574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.log(1.8488e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ae0fc-1d45-4e30-87d8-dc54d9434fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(model.snnl_criterion.summed_masked_pick_probability) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fbe4e-0ca6-492f-9ced-0ff8b60d17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import .normalize(\n",
    "# a = torch.Tensor([[2],[1],[2],[3],[2],[9]]) \n",
    "a.shape \n",
    "# b = torch.Tensor([[3],[4],[2],[4],[5],[5]])\n",
    "a = torch.Tensor([2,1,2,3,2,9])\n",
    "b = torch.Tensor([3,4,2,4,5,5])\n",
    "product = torch.matmul(a, b)\n",
    "product\n",
    "normalized_a = torch.nn.functional.normalize(a, dim=0, p=2)\n",
    "normalized_b = torch.nn.functional.normalize(b, dim=0, p=2)\n",
    "normalized_a\n",
    "normalized_b\n",
    "normalized_b = torch.conj(normalized_b).T\n",
    "normalized_b\n",
    "product = torch.matmul(normalized_a, normalized_b)\n",
    "product\n",
    "distance_matrix = torch.sub(torch.tensor(1.0), product)\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73a3b2-b5ac-475c-ad51-bebeb5f898c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n",
    "a.shape\n",
    "torch.cdist(a, a, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034160c-c7bc-4f97-a86b-18ce420ed593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cf31b-692e-413d-bb07-81b8333b9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_epoch, epochs, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48c90f-7fe4-4a1e-8b16-72de35fbdadd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e761a-a327-4781-bafb-ede5ea433193",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attributes = model.__dict__\n",
    "for key, value in model_attributes.items():\n",
    "    if isinstance(value, List) or \"test_accuracy\" in key:\n",
    "        print(f\"{key:25s} {type(value)}  {len(value):7d}  {value[:5]}\")\n",
    "# model_attributes['temperature_gradients'] \n",
    "temp_gradients = np.array(model_attributes['temperature_gradients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c8500-8722-44ec-ae22-b371c9add16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_snn_loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcf5d4f2-0f30-4ddc-90a0-5bd477f77679",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb528be-02f9-4cb5-98da-fa6135e29460",
   "metadata": {},
   "outputs": [],
   "source": [
    "for st in range(0,len(temp_gradients), 1000):\n",
    "    end = st + 1000\n",
    "    print(f\" {st:5d} - {end:5d}  min: {temp_gradients[st:end].min():9e}   max: {temp_gradients[st:end].max():9e}    avg: {temp_gradients[st:end].mean():9e}   std: {temp_gradients[st:end].std():9e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1332fa9-ef79-49e8-a8b8-ec7de0f31945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(model.train_snn_loss);\n",
    "_ = plt.title(f'train_snn_loss - {epochs} epochs');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c1165-a1d7-43b2-8a66-dd3b8fa22de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(model.train_snn_loss)\n",
    "# plt.title(f'train_snn_loss - {epochs} epochs')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf75d0-3aad-4dee-967a-2b55f60e6ec4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(model.train_temp_hist);\n",
    "_ = plt.title(f'train_temperature_hist - {epochs} epochs');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a9004-67b1-4e3c-9629-3a7506a2423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(model.temperature_gradients)\n",
    "_ = plt.title(f'train_temperature_gradients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede60c29-427b-48ff-81e5-82187059786f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(model.train_temp_grad_hist);\n",
    "_ = plt.title(f\"train_temperature_gradient_hist - {epochs} epochs\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8a6c6-6738-4027-b2c4-b882db0bcddc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot Temperature Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf94df-8df1-4bd7-8dad-c20b24b7d643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(model.train_temp_grad_hist[:38])\n",
    "plt.title(f'train_temperature_gradient_hist')\n",
    "plt.show()\n",
    "plt.plot(model.train_temp_grad_hist[38:42])\n",
    "plt.title(f'train_temperature_gradient_hist')\n",
    "plt.show()\n",
    "plt.plot(model.train_temp_grad_hist[42:])\n",
    "plt.title(f'train_temperature_gradient_hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0fa59-6378-4ccf-8d5f-82fcd8c7f273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(model.train_loss)\n",
    "\n",
    "plt.plot(model.train_temp_hist)\n",
    "plt.title(f'train_temperature_hist')\n",
    "plt.show()\n",
    "plt.plot(model.train_temp_grad_hist[:38])\n",
    "plt.title(f'train_temperature_gradient_hist')\n",
    "plt.show()\n",
    "plt.plot(model.train_temp_grad_hist[38:42])\n",
    "plt.title(f'train_temperature_gradient_hist')\n",
    "plt.show()\n",
    "plt.plot(model.train_temp_grad_hist[42:])\n",
    "plt.title(f'train_temperature_gradient_hist')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5b0ee-8af2-49da-8a6a-0d15e635de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.temperature_gradients)\n",
    "plt.show()\n",
    "plt.plot(model.temperature_gradients[:23600])\n",
    "plt.show()\n",
    "plt.plot(model.temperature_gradients[:43600])\n",
    "plt.show()\n",
    "plt.plot(model.temperature_gradients[43600:48600])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model.temperature_gradients[250000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b28f65-394f-49f9-aa69-87651b2cd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show()\n",
    "# plt.plot(model.temperature_gradients[43600:44600])\n",
    "# plt.show()\n",
    "# plt.plot(model.temperature_gradients[44600:45600])\n",
    "# plt.show()\n",
    "# plt.plot(model.temperature_gradients[45600:46600])\n",
    "# plt.show()\n",
    "# plt.plot(model.temperature_gradients[46600:47600])\n",
    "# plt.show()\n",
    "# plt.plot(model.temperature_gradients[47600:47700])\n",
    "# plt.show()\n",
    "# plt.plot(model.temperature_gradients[47700:47800])\n",
    "# plt.show()\n",
    "plt.plot(model.temperature_gradients[47800:47900])\n",
    "plt.show()\n",
    "# plt.plot(model.temperature_gradients[47900:48100])\n",
    "# plt.show()\n",
    "# plt.plot(model.temperature_gradients[48100:48300])\n",
    "# plt.show()\n",
    "# plt.plot(model.temperature_gradients[48300:48600])\n",
    "# plt.show()\n",
    "# plt.plot(model.temperature_gradients[48600:49600])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363250e6-c044-4383-af1d-6bf40e1a0a7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save model results and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5ddefad-a4e6-480c-85a3-acf398d10e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T04:58:52.175740Z",
     "iopub.status.busy": "2024-04-06T04:58:52.175264Z",
     "iopub.status.idle": "2024-04-06T04:58:52.214328Z",
     "shell.execute_reply": "2024-04-06T04:58:52.213769Z",
     "shell.execute_reply.started": "2024-04-06T04:58:52.175697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to DNN_snnl_lrn_temp_epoch_150_cpb_100_clipgrad_factor_10_2024_04_06_065852\n"
     ]
    }
   ],
   "source": [
    "# import datetime.timezone\n",
    "# datetime.timetz()\n",
    "timestamp = datetime.now().strftime('%Y_%m_%d_%H%M%S')\n",
    "filename = f\"DNN_{args.model.lower()}_lrn_temp_epoch_{epochs}_cpb_{compounds_per_batch}_clipgrad_factor_{10}_{timestamp}\"\n",
    "print(f\"saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96a4317a-5573-4d22-9df1-5faee3406e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T04:58:56.238148Z",
     "iopub.status.busy": "2024-04-06T04:58:56.237708Z",
     "iopub.status.idle": "2024-04-06T04:58:56.473143Z",
     "shell.execute_reply": "2024-04-06T04:58:56.472476Z",
     "shell.execute_reply.started": "2024-04-06T04:58:56.238108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model Results exported to results/DNN_snnl_lrn_temp_epoch_150_cpb_100_clipgrad_factor_10_2024_04_06_065852.json.\n",
      "[INFO] Model exported to examples/export/DNN_snnl_lrn_temp_epoch_150_cpb_100_clipgrad_factor_10_2024_04_06_065852.pt.\n",
      "[INFO] Model exported to ckpts/DNN_snnl_lrn_temp_epoch_150_cpb_100_clipgrad_factor_10_2024_04_06_065852.pt.\n"
     ]
    }
   ],
   "source": [
    "export_results(model=model, filename=filename)\n",
    "\n",
    "save_model(model, filename)\n",
    "\n",
    "_save_checkpoint(epochs, model, filename, update_latest=False, update_best=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129f797-8c07-4d45-be1b-9ddb19af3576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T00:58:52.961399Z",
     "iopub.status.busy": "2024-04-01T00:58:52.960920Z"
    },
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e5f66-a951-4142-8724-7da8888d0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snnl.utils import load_model, _load_previous_state, import_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04195192-de0a-4438-9b54-e991760b7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"DNN_snnl_withstatictemp_epoch400_2024_04_01_073006.pt\"\n",
    "fn = \"DNN_snnl_withlearning_temp_epoch250_2024_04_02_173536_model_epoch_250.pth\"\n",
    "model_clone = load_model(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85609359-e0ff-478f-807a-b72d62eb13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09a653-d2ac-47ba-b409-5179b10267ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = \"DNN_snnl_withstatictemp_epoch400_2024_04_01_073006.pt\"\n",
    "fn = \"DNN_snnl_withlearning_temp_epoch250_2024_04_02_173536_model_epoch_250.pth\"\n",
    "model, next_epoch = _load_previous_state(model, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639557c-9a64-4a66-bbaf-981b42d5023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_epoch\n",
    "model_clone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24333255-b53a-4780-b7db-50bdb92c5a4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T00:58:52.961399Z",
     "iopub.status.busy": "2024-04-01T00:58:52.960920Z"
    }
   },
   "source": [
    "#### Load model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b5f9d-3b1b-43b0-8363-31ba00f39965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = \"DNN_snnl_withstatictemp_epoch400_2024_04_01_073006.pt\"\n",
    "fn = \"DNN_snnl_withlearning_temp_epoch250_2024_04_02_173536.json\"\n",
    "\n",
    "results = import_results(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9a677-007c-4bd7-8398-916eb4724013",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()\n",
    "for k in results.keys():\n",
    "    print(len(results[k]))\n",
    "    model.__dict__[k] = results[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284c33e-8448-4c0a-a403-8dc1d8cc536d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdb486-c206-4a1c-b259-3a53fae9455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'cellpainting':\n",
    "    print(f\" load {dataset}\")\n",
    "    test_dataset = CellpaintingDataset(test = True, **cellpainting_args)\n",
    "    test_loader = InfiniteDataLoader(dataset=test_dataset, batch_size=batch_size, shuffle = False, num_workers = 0, collate_fn = train_dataset.custom_collate_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f91dca-5f0d-4ff9-9f8e-1707db9b833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(test_loader):\n",
    "    print(idx, batch[0].shape, batch[1].shape,batch[2].shape,batch[3].shape,batch[4].shape)\n",
    "    # test_features = batch[0].reshape(-1, 784)\n",
    "    # print(batch[1])\n",
    "    # display_cellpainting_batch(idx, batch)\n",
    "    # train_batch_id +=1\n",
    "    # if train_batch_id ==3 :\n",
    "        # break\n",
    "    # if idx >= 24:\n",
    "        # break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce36c0b-6fd4-4454-8ceb-214284dd6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features = test_dataset.dataa.reshape(-1, 784) / 255.0\n",
    "test_features = batch[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebac12-779c-4da2-995c-b4ecaed34828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d51865-d440-4488-b690-45bdf8a91c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_features)\n",
    "model.test_accuracy = accuracy(y_true=test_dataset.targets, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c72a58-de2e-43d9-ad00-725552a38ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy: {model.test_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d7cd53-2ef8-4c46-a509-e3443f1eae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy: {model.test_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80b138-e509-48bc-853f-297f1a55a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy: {model.test_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8338a-e430-44e7-b82d-96c3fa70cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"DNN-{args.model.lower()}-{args.seed}.json\"\n",
    "export_results(model=model, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616fdcb-52ea-4743-ace6-201a48fc68ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "333adcfa-4e1a-4e84-b837-14dd762ba6a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `epoch_train(self, data_loader, epoch)` - STEP BY STEP\n",
    "\n",
    "    def epoch_train(self, data_loader: torch.utils.data.DataLoader, epoch: int = None) -> Tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ca19d-bcba-4275-a6e3-c51f703af1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.snnl_criterion = SNNLoss(\n",
    "        mode=model.mode,\n",
    "        criterion=model.primary_criterion,\n",
    "        factor=model.factor,\n",
    "        temperature=model.temperature,\n",
    "        use_annealing=model.use_annealing,\n",
    "        use_sum=model.use_sum,\n",
    "        # unsupervised=model.unsupervised,\n",
    "        unsupervised=True,\n",
    "        code_units=model.code_units,\n",
    "        sample_size = model.sample_size,\n",
    "        stability_epsilon=model.stability_epsilon,\n",
    ")\n",
    "model.unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d69ccd-3c4d-425a-9e2e-f1df7c3b3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model.use_snnl:\n",
    "# if model.name == \"DNN\" or model.name == \"CNN\":\n",
    "epoch_loss = 0\n",
    "epoch_ttl_loss = 0\n",
    "epoch_primary_loss = 0\n",
    "epoch_snn_loss = 0\n",
    "epoch_accuracy = 0\n",
    "\n",
    "# model.snnl_criterion.factor \n",
    "# model.snnl_criterion.factor = 2\n",
    "# model.snnl_criterion.factor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109187f-0dce-4eda-8ba7-214f353ac877",
   "metadata": {},
   "source": [
    "#### Loop through data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0194b-ea0c-4c3a-ae30-3664c00099da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch_count, (batch_features, batch_labels, _, _, _) in enumerate(train_loader):\n",
    "    if batch_count  == 71:\n",
    "        break\n",
    "    # ,_,_,batch_features, batch_labels = next(iter(data_loader))\n",
    "    # batch_features, batch_labels = next(iter(data_loader))\n",
    "\n",
    "    # if model.name in [\"Autoencoder\", \"DNN\"]:\n",
    "        # batch_features = batch_features.view(batch_features.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f478fd-792a-496a-93b3-b9439d30869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a511f-51f4-4d81-8041-029f70d7654f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "batch_features = batch_features.to(model.device)\n",
    "batch_labels = batch_labels.to(model.device)\n",
    "    \n",
    "model.optimizer.zero_grad()\n",
    "    \n",
    "outputs = model.forward(features=batch_features)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a7016-15af-4fdb-b3f2-7ec7f8dabac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c472c4f-a10c-472c-9f64-09aea53440be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    if model.use_snnl:\n",
    "        train_loss, primary_loss, snn_loss = model.snnl_criterion(\n",
    "            model=model,\n",
    "            outputs=outputs,\n",
    "            features=batch_features,\n",
    "            labels=batch_labels,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        epoch_loss += train_loss.item()\n",
    "        epoch_snn_loss += snn_loss.item()\n",
    "        epoch_primary_loss += primary_loss.item()\n",
    "    else:\n",
    "        # if not model.use_snnl:\n",
    "        print(f\"Model not using SNNL\")\n",
    "        train_loss = model.criterion(outputs, batch_labels if model.name == \"DNN\" or model.name == \"CNN\"  else batch_features,)\n",
    "        epoch_loss += train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f295bda-c719-4fff-b7ca-51ae611ccacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "    if model.name == \"DNN\" or model.name == \"CNN\":\n",
    "        train_accuracy = (outputs.argmax(1) == batch_labels).sum().item() / len(batch_labels)\n",
    "        epoch_accuracy += train_accuracy\n",
    "    else:\n",
    "        train_accuracy =  0\n",
    "        epoch_accuracy += train_accuracy\n",
    "    \n",
    "    train_loss.backward()\n",
    "    model.optimizer.step()\n",
    "    \n",
    "    if model.use_snnl and model.temperature is not None:\n",
    "        model.optimize_temperature()\n",
    "    \n",
    "    # print(outputs.shape)\n",
    "    # print(batch_labels.shape)\n",
    "    print(f\" batch:{batch_count} - train : loss    {train_loss:10.6f}   primary loss: {primary_loss:10.6f}    SNN loss: {snn_loss*model.snnl_criterion.factor:10.6f}   (loss: {snn_loss:10.6f} * factor: {model.snnl_criterion.factor})  \"\n",
    "          f\"  temp: {model.temperature.data} snnl_temp: {model.snnl_criterion.temperature.data}\")    \n",
    "    # print(f\" epoch : loss    {epoch_loss:10.6f}   primary_loss: {epoch_primary_loss:10.6f}    SNN loss: {epoch_snn_loss:10.6f}   factor: {model.factor}\")\n",
    "    # print(f\" batch_accuracy         {train_accuracy}   epoch_accuracy         {epoch_accuracy}\")\n",
    "    break\n",
    "#### End of dataloader loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110abcf-252c-4480-bd36-00e1346ca24a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\" loop ended - batch_count: {batch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57076e3e-ecd7-4981-b6b6-10dd73b8bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_loss\n",
    "primary_loss.shape, primary_loss.size(), primary_loss.data, primary_loss.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e928131-adf6-4100-a66b-548e12544e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = torch.zeros(0,requires_grad = True, dtype=torch.float32)\n",
    "p1 = torch.tensor(0,requires_grad = True, dtype=torch.float32, device = model.device)\n",
    "pp.shape, pp.size(), pp.data, pp.ndim\n",
    "p1.shape, p1.size(), p1.data, p1.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae028f-8b82-4635-b110-a294788ed55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data_loader = batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b840a1c-78b7-4c85-bd79-1f445323449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data_loader\n",
    "epoch_loss, epoch_loss/len_data_loader\n",
    "epoch_snn_loss * model.snnl_criterion.factor, (epoch_snn_loss * model.snnl_criterion.factor)/len_data_loader, epoch_snn_loss\n",
    "epoch_primary_loss, epoch_primary_loss/len_data_loader\n",
    "epoch_accuracy / len_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62eb13-604d-41fd-b6a7-31fa2c3f7056",
   "metadata": {},
   "source": [
    "## SNNL Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3d1ff-7509-47b7-b986-3f6b232ff165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1 = torch.randn(100, 128)\n",
    "# input2 = torch.randn(100, 128)\n",
    "# cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "# tmp1 = cos(input1, input2)\n",
    "# tmp1.shape\n",
    "# # tmp1\n",
    "\n",
    "# value.shape\n",
    "\n",
    "# tmp1 = torch.cdist(value, value, p=2)\n",
    "# cosim = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "# tmp1 = cosim(value, value.T)\n",
    "# tmp1.shape\n",
    "# tmp1\n",
    "# tmp1.min(), tmp1.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2d86d-149c-4c52-9bc8-bf6f4440e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "snnl =SNNLoss(\n",
    "                mode=model.mode,\n",
    "                criterion=model.primary_criterion,\n",
    "                factor=model.factor,\n",
    "                temperature=model.temperature,\n",
    "                use_annealing=model.use_annealing,\n",
    "                use_sum=model.use_sum,\n",
    "                code_units=model.code_units,\n",
    "                sample_size = model.sample_size,\n",
    "                stability_epsilon=model.stability_epsilon,\n",
    "                unsupervised=model.unsupervised,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031d50b-56b8-4cf2-a0ff-124f064b8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "snnl.temperature = 8.762743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8294a6-0992-44c9-adf3-c4dbec0ba543",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.temperature\n",
    "snnl.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05387b-dd68-4d17-ae71-94da07de91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = snnl.compute_activations(model=model, features=batch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1ae97-fe84-4ef3-b9fb-0c43c309ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 1\n",
    "value = activations[1]\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3aede6-cfc1-48a5-b031-5a1902385982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_cosine_distance(features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns the pairwise cosine distance between two copies\n",
    "    of the features matrix.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    features: torch.Tensor\n",
    "        The input features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance_matrix: torch.Tensor\n",
    "        The pairwise cosine distance matrix.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> from snnl import SNNLoss\n",
    "    >>> _ = torch.manual_seed(42)\n",
    "    >>> a = torch.rand((4, 2))\n",
    "    >>> snnl = SNNLoss(temperature=1.0)\n",
    "    >>> snnl.pairwise_cosine_distance(a)\n",
    "    tensor([[1.1921e-07, 7.4125e-02, 1.8179e-02, 1.0152e-01],\n",
    "            [7.4125e-02, 1.1921e-07, 1.9241e-02, 2.2473e-03],\n",
    "            [1.8179e-02, 1.9241e-02, 1.1921e-07, 3.4526e-02],\n",
    "            [1.0152e-01, 2.2473e-03, 3.4526e-02, 0.0000e+00]])\n",
    "    \"\"\"\n",
    "    a, b = features.clone(), features.clone()\n",
    "    normalized_a = torch.nn.functional.normalize(a, dim=1, p=2)\n",
    "    normalized_b = torch.nn.functional.normalize(b, dim=1, p=2)\n",
    "    # normalized_b = torch.conj(normalized_b).T\n",
    "    product = torch.matmul(normalized_a, normalized_b.T)\n",
    "    distance_matrix = torch.sub(torch.tensor(1.0), product)\n",
    "    return distance_matrix\n",
    "\n",
    "    def normalize_distance_matrix(\n",
    "        self,\n",
    "        features: torch.Tensor,\n",
    "        distance_matrix: torch.Tensor,\n",
    "        device: torch.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        pairwise_distance_matrix = torch.exp( -(distance_matrix * self.temperature)) - torch.eye(features.shape[0]).to(device)\n",
    "        \n",
    "        return pairwise_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5033b908-5888-4ac9-b244-58d0144a5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_cos_distance(A, B):\n",
    "    \n",
    "    \"\"\"Pairwise cosine distance between two matrices.\n",
    "    :param A: a matrix.\n",
    "    :param B: a matrix.\n",
    "    :returns: A tensor for the pairwise cosine between A and B.\n",
    "    \"\"\"\n",
    "    normalized_A = torch.nn.functional.normalize(A, dim=1)\n",
    "    normalized_B = torch.nn.functional.normalize(B, dim=1)\n",
    "    prod = torch.matmul(normalized_A, normalized_B.transpose(-2, -1).conj())\n",
    "    return 1 - prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3bfb3-cc9f-4555-b3c6-24029d1eccc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### V1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b317a9-4d55-4556-ba2f-db26ce7d8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n','-'*80)\n",
    "print('pairwise_cos_distance')\n",
    "print('-'*80)\n",
    "\n",
    "tmp1 =  pairwise_cosine_distance(value)\n",
    "tmp1.shape\n",
    "tmp1\n",
    "tmp1.min(), tmp1.argmin()\n",
    "\n",
    "\n",
    "print('\\n','-'*80)\n",
    "print('pairwise_distance_matrix')\n",
    "print('-'*80)\n",
    "\n",
    "tmp2 = torch.exp(-(tmp1 / snnl.temperature)) - torch.eye(batch_features.shape[0]).to(model.device) \n",
    "tmp2.shape\n",
    "tmp2\n",
    "tmp2.min(), tmp2.argmin()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e35426-6e71-4cc8-8f40-0f9b7229b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "snnl.distance_matrix = snnl.pairwise_cosine_distance(features = value)\n",
    "snnl.distance_matrix.shape\n",
    "snnl.distance_matrix\n",
    "snnl.distance_matrix.min(), snnl.distance_matrix.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008afbc-b247-47db-bef1-85c5e18859fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "snnl.pairwise_distance_matrix = snnl.normalize_distance_matrix(features = value, distance_matrix = snnl.distance_matrix, device = 'cuda:0')\n",
    "snnl.pairwise_distance_matrix\n",
    "snnl.pairwise_distance_matrix.shape   \n",
    "snnl.pairwise_distance_matrix.min(), snnl.pairwise_distance_matrix.argmin()   \n",
    "# pairwise_distance_matrix\n",
    "# tmp1 = torch.exp(-(snnl.distance_matrix / snnl.temperature)) \n",
    "# tmp1.shape\n",
    "# tmp1.max(), tmp1.argmax()\n",
    "# tmp1.min(), tmp1.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c97b3-19fc-4f41-8501-bb67c27c8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "snnl.pick_probability = snnl.compute_sampling_probability(snnl.pairwise_distance_matrix)\n",
    "snnl.pick_probability.shape   \n",
    "snnl.pick_probability\n",
    "# snnl.pick_probability.sum(0), snnl.pick_probability.sum(1) \n",
    "snnl.pick_probability.min(), snnl.pick_probability.argmin()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b00c2-7e99-4ca8-84dd-26582a8e30c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snnl.summed_masked_pick_probability = snnl.mask_sampling_probability(labels = batch_labels, sampling_probability = snnl.pick_probability, device = 'cuda:0')\n",
    "snnl.summed_masked_pick_probability.shape\n",
    "snnl.summed_masked_pick_probability.sum(0)\n",
    "snnl.summed_masked_pick_probability.min(), snnl.summed_masked_pick_probability.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1092e-3039-48e3-8bfd-a25c1e320764",
   "metadata": {},
   "outputs": [],
   "source": [
    "snnl_loss = torch.mean( -torch.log(snnl.stability_epsilon + snnl.summed_masked_pick_probability))\n",
    "snnl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df737118-bb01-4c6b-a3a5-9c91f3dbd1fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### V2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894ced5b-a476-414d-979d-65aae5f0811e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T22:41:13.669896Z",
     "iopub.status.busy": "2024-04-05T22:41:13.669404Z",
     "iopub.status.idle": "2024-04-05T22:41:13.697638Z",
     "shell.execute_reply": "2024-04-05T22:41:13.696983Z",
     "shell.execute_reply.started": "2024-04-05T22:41:13.669852Z"
    }
   },
   "outputs": [],
   "source": [
    "from snnl.losses import SNNLCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b3c7e-f26d-490e-a37f-f150efb7ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "snnlV2 = SNNLCrossEntropy(model,\n",
    "                         temperature=100.,\n",
    "                         layer_names=[0,1,2,3,4],\n",
    "                         factor=-10.,\n",
    "                         optimize_temperature=True,\n",
    "                         cos_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb12d01-8084-48f5-a930-a67248371fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snnlV2.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dde7c-48a7-41b2-ad81-b09cb20de92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = snnlV2.SNNL(x = value.to('cpu') ,\n",
    "              y = snnl.sample_labels.to('cpu') ) \n",
    "# snnl.sample_labels\n",
    "tmp1\n",
    "# torch.arange(batch_labels.shape[0]).to('cuda:0') // snnl.sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbc393-b33e-446e-8cbe-15ebb5ea606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = snnl.temperature\n",
    "temp\n",
    "sample_labels = snnl.sample_labels.to('cpu') \n",
    "# sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18ee91-e926-4daa-861d-050b5e757178",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n','-'*80)\n",
    "print('pairwose_cos_distance')\n",
    "print('-'*80)\n",
    "tmp2 = snnlV2.pairwise_cos_distance(value, value)\n",
    "tmp2.shape\n",
    "tmp2\n",
    "tmp2.min(), tmp2.argmin()\n",
    "\n",
    "print('\\n','-'*80)\n",
    "print('torch.exp(-(distance_matrix / temp))')\n",
    "print('-'*80)\n",
    "tmp3 = torch.exp(-(tmp2 / temp))\n",
    "tmp3.shape\n",
    "tmp3\n",
    "tmp3.min(), tmp3.argmin()\n",
    "\n",
    "print('\\n','-'*80)\n",
    "print(' torch.exp(-(distance_matrix / temp)) - torch.eye(value.shape[0] ')\n",
    "print('-'*80)\n",
    "tmp4 = tmp3 - torch.eye(value.shape[0]).to('cuda')\n",
    "tmp4.shape\n",
    "tmp4\n",
    "tmp4.min(), tmp4.argmin()\n",
    "\n",
    "print('\\n','-'*80)\n",
    "print(' PickProbability --- tmp4 / (SNNLCrossEntropy.STABILITY_EPS + tmp4.sum(axis=1).unsqueeze(1))')\n",
    "print('-'*80) \n",
    "tmp5 = tmp4 / (SNNLCrossEntropy.STABILITY_EPS + tmp4.sum(axis=1).unsqueeze(1))\n",
    "tmp5.shape\n",
    "tmp5[:10,:10]\n",
    "tmp5\n",
    "tmp5.min(), tmp5.argmin()\n",
    "\n",
    "print('\\n','-'*80)\n",
    "print(' MaskedPickProbability ')\n",
    "print('-'*80) \n",
    "tmp6 = tmp5.to('cpu') * snnlV2.same_label_mask(sample_labels, sample_labels )\n",
    "tmp5.shape\n",
    "tmp6[:10,:10]\n",
    "tmp6\n",
    "tmp6.min(), tmp6.argmin()\n",
    "\n",
    "print('\\n','-'*80)\n",
    "print(' SummedMaskedPickProbability ')\n",
    "print('-'*80) \n",
    "tmp7 = tmp6.sum(axis=1)\n",
    "tmp7.shape\n",
    "tmp7\n",
    "tmp7.min(), tmp7.argmin()\n",
    "\n",
    "\n",
    "print('\\n','-'*80)\n",
    "print(' SummedMaskedPickProbability ')\n",
    "print('-'*80) \n",
    "tmp8 =  -torch.log(SNNLCrossEntropy.STABILITY_EPS + tmp7).mean()\n",
    "tmp8.shape\n",
    "tmp8\n",
    "tmp8.min(), tmp8.argmin()\n",
    "\n",
    "\n",
    "# summed_masked_pick_prob = SNNLCrossEntropy.masked_pick_probability(x, y, temp, cos_distance).sum(axis=1)\n",
    "# snnlV2.same_label_mask(sample_labels, sample_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d948f9f-13de-48a9-8fe1-650857b1951b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a4c6d52-8be3-4480-9406-c3ad08546cbd",
   "metadata": {},
   "source": [
    " #### Print values used in snnl loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902d042-5784-4ed6-a702-6ece9e389de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs.shape\n",
    "# outputs[0].data\n",
    "# outputs.argmax(1)\n",
    "batch_labels\n",
    "\n",
    "# batch_labels[1]\n",
    "# train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be601564-e342-4e36-b98e-cb4b84fdd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(linewidth=180)\n",
    "\n",
    "# model.snnl_criterion.distance_matrix[:10,:10]\n",
    "\n",
    "# model.snnl_criterion.distance_matrix[:10,:10]\n",
    "\n",
    "# model.snnl_criterion.pairwise_distance_matrix[:10,:10]\n",
    "\n",
    "# model.snnl_criterion.pick_probability[:10,:10]\n",
    "\n",
    "# model.snnl_criterion.masking_matrix[:10,:10]\n",
    "\n",
    "# model.snnl_criterion.masked_pick_probability[:10,:10]\n",
    "\n",
    "# model.snnl_criterion.summed_masked_pick_probability[:10]\n",
    "\n",
    "# tmp1 = torch.eq(model.snnl_criterion.sample_labels, model.snnl_criterion.sample_labels.unsqueeze(1)).float()\n",
    "# tmp1.shape\n",
    "# tmp1[:12,:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a17e9-06b2-4f44-8d57-4861900feb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (outputs.argmax(1) == batch_labels).sum().item()\n",
    "# len(batch_labels)\n",
    "# len(data_loader)\n",
    "\n",
    "# activations = dict()\n",
    "# layers = model.layers\n",
    "# for index, layer in enumerate(layers):\n",
    "#     if index == 0:\n",
    "#         activations[index] = layer(batch_features)\n",
    "#     else:\n",
    "#         activations[index] = layer(activations[index - 1])\n",
    "# for i in activations:\n",
    "#     print(activations[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57c906-31f5-4f9c-af5d-7e6d64565593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_data_loader = batch_count -1\n",
    "epoch_loss /= len_data_loader\n",
    "\n",
    "if model.name in [\"DNN\", \"CNN\"]:\n",
    "    epoch_accuracy /= len_data_loader ## len(data_loader)\n",
    "\n",
    "if model.use_snnl:\n",
    "    epoch_snn_loss = epoch_snn_loss * model.snnl_criterion.factor / len_data_loader ## len(data_loader)\n",
    "    epoch_primary_loss /=  len_data_loader  ##  len(data_loader)\n",
    "    \n",
    "    if model.name == \"DNN\" or model.name == \"CNN\":\n",
    "        print(f\" epoch_loss: {epoch_loss},  epoch_snn_loss: {epoch_snn_loss}, epoch_primaey_loss: {epoch_primary_loss}, accuracy: {epoch_accuracy} \")\n",
    "        a,b = (epoch_loss, epoch_snn_loss, epoch_primary_loss), epoch_accuracy\n",
    "    else:\n",
    "        print(f\" epoch_loss: {epoch_loss},  epoch_snn_loss: {epoch_snn_loss}, epoch_primary_loss: {epoch_primary_loss} \")\n",
    "        a =  (epoch_loss, epoch_snn_loss, epoch_primary_loss)\n",
    "else:\n",
    "    if model.name == \"DNN\" or model.name == \"CNN\":\n",
    "        print(f\" epoch_loss: {epoch_loss}, accuracy: {epoch_accuracy} \")\n",
    "        a , b = epoch_loss, epoch_accuracy\n",
    "    else:\n",
    "        print(f\" epoch_loss: {epoch_loss},\")\n",
    "        a =  epoch_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptsnnl]",
   "language": "python",
   "name": "conda-env-ptsnnl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
